{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import tensorflow as tf\n",
    "from utils.data_preprocess import load_data\n",
    "from utils.module import model_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from utils.model_evaluation import plot_test_pred\n",
    "import pandas as pd\n",
    "\n",
    "# Sandwiched dropout layer to the hyperparameter tuned model and did the predictions 1000 times without training the model again\n",
    "# Sandwiched dropout layer to the hyperparameter tuned model and did the predictions 1000 times after training the model again\n",
    "# During hyperparameter tuning, it is ensured that a dropout layer is there after a dense layer. This model is directly used for uncertainity quanitification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_leakage, two_leakage = load_data()\n",
    "# two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "# single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "data['x2'] = data['x2'].replace(np.nan, 0)\n",
    "data['y2'] = data['y2'].replace(np.nan, 0)\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual', 'total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', \"leak_2\"]]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', \"leak_2\"], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "\n",
    "# sort x1, y1 and x2, y2. coordinates with lowest x will take the position of x1\n",
    "def coords_swap(y1):\n",
    "    s = y1['x2'] < y1['x1']\n",
    "    y1.loc[s, ['x1','x2']] = y1.loc[s, ['x2','x1']].values\n",
    "    y1.loc[s, ['y1','y2']] = y1.loc[s, ['y2','y1']].values\n",
    "    return y1\n",
    "\n",
    "y1_test_unsc = y_test\n",
    "\n",
    "y1_data = [y_train, y_val, y_test]\n",
    "y1_data_types = ['y_train', 'y_val', 'y_test']\n",
    "for y1_data_types, y1 in zip(y1_data_types, y1_data):\n",
    "    y1_data_types = coords_swap(y1)\n",
    "\n",
    "y1_train = y_train[['x1', 'y1']]\n",
    "y1_test = y_test[['x1', 'y1']]\n",
    "y1_val = y_val[['x1', 'y1']]\n",
    "\n",
    "y2_train = y_train[['x2', 'y2']]\n",
    "y2_test = y_test[['x2', 'y2']]\n",
    "y2_val = y_val[['x2', 'y2']]\n",
    "\n",
    "\n",
    "y3_train = y_train[[\"leak_2\"]]\n",
    "y3_test = y_test[[\"leak_2\"]]\n",
    "y3_val = y_val[[\"leak_2\"]]\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "y3_columns = y3_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "scaler_coords2 = StandardScaler()\n",
    "y2_train = scaler_coords2.fit_transform(y2_train)\n",
    "y2_test = scaler_coords2.transform(y2_test)\n",
    "y2_val = scaler_coords2.transform(y2_val)\n",
    "\n",
    "y2_train = pd.DataFrame(y2_train, columns=y2_columns)\n",
    "y2_test = pd.DataFrame(y2_test, columns=y2_columns)\n",
    "y2_val = pd.DataFrame(y2_val, columns=y2_columns)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_columns)\n",
    "\n",
    "# y_train = [y1_train, y2_train]\n",
    "# y_val = [y1_val, y2_val]\n",
    "# y_test = [y1_test, y2_test]\n",
    "\n",
    "y3_train = y3_train.reset_index().drop(columns='sample_number')\n",
    "y3_val = y3_val.reset_index().drop(columns='sample_number')\n",
    "y3_test = y3_test.reset_index().drop(columns='sample_number')\n",
    "\n",
    "y_train_all = pd.concat([y1_train, y2_train, y3_train], axis=1)\n",
    "y_test_all = pd.concat([y1_test, y2_test, y3_test], axis=1)\n",
    "y_val_all = pd.concat([y1_val, y2_val, y3_val], axis=1)\n",
    "\n",
    "X_train_np, y1_train_np, y2_train_np, y3_train_np = X_train.values, y1_train.values, y2_train.values, y3_train.values\n",
    "X_val_np, y1_val_np, y2_val_np, y3_val_np = X_val.values, y1_val.values, y2_val.values, y3_val.values\n",
    "X_test_np, y1_test_np, y2_test_np, y3_test_np = X_test.values, y1_test.values, y2_test.values, y3_test.values\n",
    "\n",
    "# Create TensorFlow datasets from NumPy arrays.\n",
    "batch_size = 32\n",
    "buffer_size = len(X_train)  # Set the buffer size to the number of training examples for full shuffling.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y1_train_np, y2_train_np , y3_train_np))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y1_val_np, y2_val_np, y3_val_np))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_np, y1_test_np, y2_test_np, y3_test_np))\n",
    "\n",
    "# Shuffle, batch, and prefetch the training dataset.\n",
    "train_dataset_org = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch the validation and test datasets.\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    mask = tf.keras.backend.cast(tf.keras.backend.not_equal(y_true, 0), tf.keras.backend.floatx())\n",
    "    mse = tf.keras.backend.mean(tf.keras.backend.square(y_true - y_pred) * mask)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 160)          1760        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          82432       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 416)          213408      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 416)          173472      ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 160)          66720       ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 160)          66720       ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 168)          27048       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 168)          27048       ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 88)           14872       ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 88)           14872       ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 72)           6408        ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 72)           6408        ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " y1 (Dense)                     (None, 2)            146         ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " y2 (Dense)                     (None, 2)            146         ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " y3 (Dense)                     (None, 1)            73          ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 701,533\n",
      "Trainable params: 701,533\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'masked_mse':masked_mse}):\n",
    "              model = tf.keras.models.load_model('saved_model/Multi_leak/experiment5/Mask_MTL_sameLoss')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"dense_8\" (type Dense).\n\nDimensions must be equal, but are 160 and 416 for '{{node dense_8/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](dense_8/Cast, dense_8/MatMul/ReadVariableOp)' with input shapes: [?,160], [416,160].\n\nCall arguments received by layer \"dense_8\" (type Dense):\n  • inputs=tf.Tensor(shape=(None, 160), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Work/studienarbeit/leakage_detection_cfrp/Multi_leakage_uncertainity_dropout.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Work/studienarbeit/leakage_detection_cfrp/Multi_leakage_uncertainity_dropout.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m stoch_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential()\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Work/studienarbeit/leakage_detection_cfrp/Multi_leakage_uncertainity_dropout.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/Work/studienarbeit/leakage_detection_cfrp/Multi_leakage_uncertainity_dropout.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     stoch_model\u001b[39m.\u001b[39;49madd(layer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Work/studienarbeit/leakage_detection_cfrp/Multi_leakage_uncertainity_dropout.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Add your intermediate layer after each existing layer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Work/studienarbeit/leakage_detection_cfrp/Multi_leakage_uncertainity_dropout.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1973\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1970\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[1;32m   1971\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1972\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m-> 1973\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[1;32m   1975\u001b[0m \u001b[39m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m \u001b[39m# TF_Operation.\u001b[39;00m\n\u001b[1;32m   1977\u001b[0m \u001b[39mif\u001b[39;00m extract_traceback:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"dense_8\" (type Dense).\n\nDimensions must be equal, but are 160 and 416 for '{{node dense_8/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](dense_8/Cast, dense_8/MatMul/ReadVariableOp)' with input shapes: [?,160], [416,160].\n\nCall arguments received by layer \"dense_8\" (type Dense):\n  • inputs=tf.Tensor(shape=(None, 160), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "dropout_prob = 0.1\n",
    "\n",
    "stoch_model = tf.keras.Sequential()\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    stoch_model.add(layer)\n",
    "    # Add your intermediate layer after each existing layer\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if i == len(model.layers)-1:\n",
    "        continue\n",
    "    intermediate_layer = tf.keras.layers.Dropout(dropout_prob)\n",
    "    stoch_model.add(intermediate_layer)\n",
    "\n",
    "# Compile the new model\n",
    "stoch_model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=model.optimizer.lr.numpy()),\n",
    "                loss=\"mse\",\n",
    "                metrics='mae')\n",
    "# Print the summary of the new model\n",
    "# stoch_model.fit(X_train, y_train, epochs=1000, validation_data = (X_val, y_val), shuffle= True)\n",
    "# stoch_model.summary()\n",
    "# model_evaluate, y_pred = model_eval(stoch_model, X_test, y_test, X_train, y_train, X_val, y_val)\n",
    "# # # # %%\n",
    "# pred=np.stack([stoch_model(X_test,training=True) \n",
    "#                for sample in range(1000)])\n",
    "# predictions_list = pred.tolist()\n",
    "# predictions_list_unsc = []\n",
    "# # print(len(predictions_list))\n",
    "# for pred in predictions_list:\n",
    "#     pred = scaler_coords.inverse_transform(pred)\n",
    "#     predictions_list_unsc.append(pred)\n",
    "# predictions__unsc = np.array(predictions_list_unsc)\n",
    "\n",
    "# # print(predictions__unsc.shape)\n",
    "# pred_mean=predictions__unsc.mean(axis=0)\n",
    "# pred_std = predictions__unsc.std(axis=0) \n",
    "# # print(pred_mean.shape, pred_std.shape)\n",
    "# # # %%\n",
    "# y_test = scaler_coords.inverse_transform(y_test)\n",
    "# plot_test_pred(y_test, pred_mean)\n",
    "# # print(pred_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Average(lst):\n",
    "#     return sum(lst)/len(lst)\n",
    "# print(Average(radius.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate 100 random 2D arrays for demonstration (replace with your own data)\n",
    "num_arrays = 1000\n",
    "array_size = (54, 2)  # Replace with the actual size of your arrays\n",
    "arrays = predictions_list_unsc\n",
    "\n",
    "# Initialize a variable to store the sum of L2 distances\n",
    "sum_of_distances = 0.0\n",
    "\n",
    "# Calculate the sum of L2 distances between all pairs of arrays\n",
    "for i in range(num_arrays):\n",
    "    for j in range(i + 1, num_arrays):  # Avoid calculating the distance for the same pairs twice\n",
    "        distance = np.linalg.norm(arrays[i] - arrays[j])  # Calculate L2 distance\n",
    "        sum_of_distances += distance\n",
    "\n",
    "# Calculate the average L2 distance\n",
    "radius = sum_of_distances / (num_arrays * (num_arrays - 1) / 2)\n",
    "\n",
    "print(\"Average L2 Distance:\", radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_std.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_range = np.arange(180, 16048, 250)\n",
    "y_range = np.arange(100, 5233, 250)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "def plot_test_pred_uncertainity(test, pred, radius):\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    \n",
    "    # plt.title(f'Sample Number {sample_number}', fontsize=20)\n",
    "    \n",
    "    # plot sensor positions\n",
    "    sensors = np.array([[2426, 70], [5480, 70], [8661, 191], [11676, 584], [13976, 917], [2603, 5163], [5723, 5163], [8417, 5103], [11646, 4740], [14641, 4391]])\n",
    "    for i in range(len(sensors)):\n",
    "        plt.scatter(sensors[i, 0], sensors[i, 1], color='tab:green', s=300)\n",
    "        if i < 5:\n",
    "            plt.text(sensors[i, 0], sensors[i, 1] - 200, 'MFC'+str(i+1), fontsize='xx-large')\n",
    "        else:\n",
    "            plt.text(sensors[i, 0], sensors[i, 1] + 350, 'MFC'+str(i+1), fontsize='xx-large')\n",
    "\n",
    "    # plot leakage positions\n",
    "    plt.scatter(X, Y, color='black', s=10)\n",
    "    # radius = radius.tolist()\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        if i != len(test)-1:\n",
    "            plt.scatter(test[i][0], test[i][1], color='tab:red', s=200, alpha= 0.7)\n",
    "            plt.scatter(pred[i][0], pred[i][1], s=radius, color='tab:blue', alpha= 0.7)\n",
    "            line = np.vstack((test[i], pred[i])).transpose()\n",
    "            plt.plot(line[0], line[1], color = 'black')\n",
    "            plt.annotate(str(i), (pred[i][0], pred[i][1] + 0.2), fontsize=15)\n",
    "            plt.annotate(str(i), (test[i][0], test[i][1] + 0.2), fontsize=15)\n",
    "        else:\n",
    "            plt.scatter(test[i][0], test[i][1], color='tab:red', s=200, label=\"true\", alpha= 0.7)\n",
    "            plt.scatter(pred[i][0], pred[i][1], s=radius, color='tab:blue', label=\"pred\", alpha= 0.7)\n",
    "            line = np.vstack((test[i], pred[i])).transpose()\n",
    "            plt.plot(line[0], line[1], color = 'black')\n",
    "            plt.annotate(str(i), (pred[i][0], pred[i][1] + 0.2), fontsize=15)\n",
    "            plt.annotate(str(i), (test[i][0], test[i][1] + 0.2), fontsize=15)\n",
    "\n",
    "    # print(X.shape)\n",
    "    legend = plt.legend(loc='upper right')\n",
    "    # legend.legendHandles[0]._legmarker.set_markersize(15)\n",
    "    # legend.legendHandles[0]._legmarker.set_alpha(1)\n",
    "    # plot wing contour\n",
    "    plot_wing_contour()\n",
    "\n",
    "    # include grid coordinate system\n",
    "    plt.hlines(0, -1000, 17000, linestyle='dashed')\n",
    "    plt.hlines(2600, -1000, 17000, linestyle='dashed')\n",
    "    plt.hlines(5233, -1000, 17000, linestyle='dashed')\n",
    "    plt.vlines(0, -1000, 6000, linestyle='dashed')\n",
    "    plt.vlines(7930, -1000, 6000, linestyle='dashed')\n",
    "    plt.vlines(16048, -1000, 6000, linestyle='dashed')\n",
    "    plt.text(-850, -75, '$y = 0$', fontsize=20)\n",
    "    plt.text(-850, 2600-75, '$y = 2600$', fontsize=20)\n",
    "    plt.text(-850, 5233-75, '$y = 5233$', fontsize=20)\n",
    "    plt.text(75, -700, '$x=0$', fontsize=20)\n",
    "    plt.text(7930+75, -700, '$x=7930$', fontsize=20)\n",
    "    plt.text(16048+75, -700, '$x=16048$', fontsize=20)\n",
    "    # plt.text(180, 5800, f'(x1, y1) = ({x1}, {y1}) = ({j1-31}, {-i1+10})', fontsize=20)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    # invert y axis\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    lgnd = plt.legend(loc=\"upper left\", numpoints=2, fontsize=15)\n",
    "\n",
    "    #change the marker size manually for both lines\n",
    "    lgnd.legendHandles[0]._sizes = [30]\n",
    "    lgnd.legendHandles[1]._sizes = [30]\n",
    "\n",
    "    plt.savefig('./results/hypermodel_results.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_wing_contour():\n",
    "    plt.plot([0, 7930], [0, 0], 'k')\n",
    "    plt.plot([7930, 16048], [0, 1149], 'k')\n",
    "    plt.plot([16048, 16048], [1149, 4386], 'k')\n",
    "    plt.plot([16048, 7843], [4386, 5233], 'k')\n",
    "    plt.plot([7843, 2493], [5233, 5233], 'k')\n",
    "    plt.plot([2493, 0], [5233, 0], 'k')\n",
    "    plt.xlim([-1000, 17000])\n",
    "    plt.ylim([-1000, 6000])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "\n",
    "\n",
    "plot_test_pred_uncertainity(y_test, pred_mean, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_range = np.arange(180, 16048, 250)\n",
    "y_range = np.arange(100, 5233, 250)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "def plot_test_pred_uncertainity(test, pred, radius, std):\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    \n",
    "    # plt.title(f'Sample Number {sample_number}', fontsize=20)\n",
    "    \n",
    "    # plot sensor positions\n",
    "    sensors = np.array([[2426, 70], [5480, 70], [8661, 191], [11676, 584], [13976, 917], [2603, 5163], [5723, 5163], [8417, 5103], [11646, 4740], [14641, 4391]])\n",
    "    for i in range(len(sensors)):\n",
    "        plt.scatter(sensors[i, 0], sensors[i, 1], color='tab:green', s=300)\n",
    "        if i < 5:\n",
    "            plt.text(sensors[i, 0], sensors[i, 1] - 200, 'MFC'+str(i+1), fontsize='xx-large')\n",
    "        else:\n",
    "            plt.text(sensors[i, 0], sensors[i, 1] + 350, 'MFC'+str(i+1), fontsize='xx-large')\n",
    "\n",
    "    # plot leakage positions\n",
    "    plt.scatter(X, Y, color='black', s=10)\n",
    "    radius = radius.tolist()\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        if i != len(test)-1:\n",
    "            plt.scatter(test[i][0], test[i][1], color='tab:red', s=200, alpha= 0.7)\n",
    "            plt.scatter(pred[i][0], pred[i][1], s=200, color='tab:blue', alpha= 0.7)\n",
    "            plt.errorbar(test[i][0], test[i][1], xerr=pred_std[i][0], yerr=pred_std[i][1], fmt='o', color='tab:grey', capsize=4)\n",
    "            line = np.vstack((test[i], pred[i])).transpose()\n",
    "            plt.plot(line[0], line[1], color = 'black')\n",
    "            plt.annotate(str(i), (pred[i][0], pred[i][1] + 0.2), fontsize=15)\n",
    "            plt.annotate(str(i), (test[i][0], test[i][1] + 0.2), fontsize=15)\n",
    "        else:\n",
    "            plt.scatter(test[i][0], test[i][1], color='tab:red', s=200, label=\"True\", alpha= 0.7)\n",
    "            plt.scatter(pred[i][0], pred[i][1], s=200, color='tab:blue', label= \"Prediction\", alpha= 0.7)\n",
    "            plt.errorbar(test[i][0], test[i][1], xerr=pred_std[i][0], yerr=pred_std[i][1], fmt='o', color='tab:grey', capsize=4, label='Uncertainty')\n",
    "            line = np.vstack((test[i], pred[i])).transpose()\n",
    "            plt.plot(line[0], line[1], color = 'black')\n",
    "            plt.annotate(str(i), (pred[i][0], pred[i][1] + 0.2), fontsize=15)\n",
    "            plt.annotate(str(i), (test[i][0], test[i][1] + 0.2), fontsize=15)\n",
    "\n",
    "    # print(X.shape)\n",
    "    # legend = plt.legend(loc='upper right')\n",
    "    # legend.legendHandles[0]._legmarker.set_markersize(15)\n",
    "    # legend.legendHandles[0]._legmarker.set_alpha(1)\n",
    "    # plot wing contour\n",
    "    plot_wing_contour()\n",
    "\n",
    "    # include grid coordinate system\n",
    "    plt.hlines(0, -1000, 17000, linestyle='dashed')\n",
    "    plt.hlines(2600, -1000, 17000, linestyle='dashed')\n",
    "    plt.hlines(5233, -1000, 17000, linestyle='dashed')\n",
    "    plt.vlines(0, -1000, 6000, linestyle='dashed')\n",
    "    plt.vlines(7930, -1000, 6000, linestyle='dashed')\n",
    "    plt.vlines(16048, -1000, 6000, linestyle='dashed')\n",
    "    plt.text(-850, -75, '$y = 0$', fontsize=20)\n",
    "    plt.text(-850, 2600-75, '$y = 2600$', fontsize=20)\n",
    "    plt.text(-850, 5233-75, '$y = 5233$', fontsize=20)\n",
    "    plt.text(75, -700, '$x=0$', fontsize=20)\n",
    "    plt.text(7930+75, -700, '$x=7930$', fontsize=20)\n",
    "    plt.text(16048+75, -700, '$x=16048$', fontsize=20)\n",
    "    # plt.text(180, 5800, f'(x1, y1) = ({x1}, {y1}) = ({j1-31}, {-i1+10})', fontsize=20)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    # invert y axis\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # lgnd = plt.legend(loc=\"upper left\", numpoints=2, fontsize=15)\n",
    "\n",
    "    #change the marker size manually for both lines\n",
    "    # lgnd.legendHandles[0]._sizes = [30]\n",
    "    # lgnd.legendHandles[1]._sizes = [30]\n",
    "\n",
    "    plt.savefig('./results/hypermodel_results.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_wing_contour():\n",
    "    plt.plot([0, 7930], [0, 0], 'k')\n",
    "    plt.plot([7930, 16048], [0, 1149], 'k')\n",
    "    plt.plot([16048, 16048], [1149, 4386], 'k')\n",
    "    plt.plot([16048, 7843], [4386, 5233], 'k')\n",
    "    plt.plot([7843, 2493], [5233, 5233], 'k')\n",
    "    plt.plot([2493, 0], [5233, 0], 'k')\n",
    "    plt.xlim([-1000, 17000])\n",
    "    plt.ylim([-1000, 6000])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "\n",
    "plot_test_pred_uncertainity(y_test, pred_mean, radius, pred_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "x_range = np.arange(180, 16048, 250)\n",
    "y_range = np.arange(100, 5233, 250)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "def plot_test_pred_uncertainity(test, pred, radius, std):\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    \n",
    "    # plt.title(f'Sample Number {sample_number}', fontsize=20)\n",
    "    \n",
    "    # plot sensor positions\n",
    "    sensors = np.array([[2426, 70], [5480, 70], [8661, 191], [11676, 584], [13976, 917], [2603, 5163], [5723, 5163], [8417, 5103], [11646, 4740], [14641, 4391]])\n",
    "    for i in range(len(sensors)):\n",
    "        plt.scatter(sensors[i, 0], sensors[i, 1], color='tab:green', s=300)\n",
    "        if i < 5:\n",
    "            plt.text(sensors[i, 0], sensors[i, 1] - 200, 'MFC'+str(i+1), fontsize='xx-large')\n",
    "        else:\n",
    "            plt.text(sensors[i, 0], sensors[i, 1] + 350, 'MFC'+str(i+1), fontsize='xx-large')\n",
    "\n",
    "    # plot leakage positions\n",
    "    plt.scatter(X, Y, color='black', s=10)\n",
    "    radius = radius.tolist()\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        if i != len(test)-1:\n",
    "            plt.scatter(test[i][0], test[i][1], color='tab:red', s=200, alpha= 0.7)\n",
    "            plt.scatter(pred[i][0], pred[i][1], s=200, color='tab:blue', alpha= 0.7)\n",
    "            line = np.vstack((test[i], pred[i])).transpose()\n",
    "            plt.plot(line[0], line[1], color = 'black')\n",
    "            plt.annotate(str(i), (pred[i][0], pred[i][1] + 0.2), fontsize=15)\n",
    "            plt.annotate(str(i), (test[i][0], test[i][1] + 0.2), fontsize=15)\n",
    "        else:\n",
    "            plt.scatter(test[i][0], test[i][1], color='tab:red', s=200, label=\"True\", alpha= 0.7)\n",
    "            plt.scatter(pred[i][0], pred[i][1], s=200, color='tab:blue', label= \"Prediction\", alpha= 0.7)\n",
    "            line = np.vstack((test[i], pred[i])).transpose()\n",
    "            plt.plot(line[0], line[1], color = 'black')\n",
    "            plt.annotate(str(i), (pred[i][0], pred[i][1] + 0.2), fontsize=15)\n",
    "            plt.annotate(str(i), (test[i][0], test[i][1] + 0.2), fontsize=15)\n",
    "\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        mean_x, mean_y = pred[i][0], pred[i][1]\n",
    "        sigma_x, sigma_y = std[i][0], std[i][1]\n",
    "\n",
    "        # Plot ellipses representing uncertainty\n",
    "        ellipse = patches.Ellipse((mean_x, mean_y), width=2*sigma_x, height=2*sigma_y, angle=0, alpha=0.2)\n",
    "        plt.gca().add_patch(ellipse)\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "    plot_wing_contour()\n",
    "\n",
    "    # include grid coordinate system\n",
    "    plt.hlines(0, -1000, 17000, linestyle='dashed')\n",
    "    plt.hlines(2600, -1000, 17000, linestyle='dashed')\n",
    "    plt.hlines(5233, -1000, 17000, linestyle='dashed')\n",
    "    plt.vlines(0, -1000, 6000, linestyle='dashed')\n",
    "    plt.vlines(7930, -1000, 6000, linestyle='dashed')\n",
    "    plt.vlines(16048, -1000, 6000, linestyle='dashed')\n",
    "    plt.text(-850, -75, '$y = 0$', fontsize=20)\n",
    "    plt.text(-850, 2600-75, '$y = 2600$', fontsize=20)\n",
    "    plt.text(-850, 5233-75, '$y = 5233$', fontsize=20)\n",
    "    plt.text(75, -700, '$x=0$', fontsize=20)\n",
    "    plt.text(7930+75, -700, '$x=7930$', fontsize=20)\n",
    "    plt.text(16048+75, -700, '$x=16048$', fontsize=20)\n",
    "    # plt.text(180, 5800, f'(x1, y1) = ({x1}, {y1}) = ({j1-31}, {-i1+10})', fontsize=20)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    # invert y axis\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # lgnd = plt.legend(loc=\"upper left\", numpoints=2, fontsize=15)\n",
    "\n",
    "    #change the marker size manually for both lines\n",
    "    # lgnd.legendHandles[0]._sizes = [30]\n",
    "    # lgnd.legendHandles[1]._sizes = [30]\n",
    "\n",
    "    plt.savefig('./results/hypermodel_results.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_wing_contour():\n",
    "    plt.plot([0, 7930], [0, 0], 'k')\n",
    "    plt.plot([7930, 16048], [0, 1149], 'k')\n",
    "    plt.plot([16048, 16048], [1149, 4386], 'k')\n",
    "    plt.plot([16048, 7843], [4386, 5233], 'k')\n",
    "    plt.plot([7843, 2493], [5233, 5233], 'k')\n",
    "    plt.plot([2493, 0], [5233, 0], 'k')\n",
    "    plt.xlim([-1000, 17000])\n",
    "    plt.ylim([-1000, 6000])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.gca().set_aspect('equal')\n",
    "\n",
    "plot_test_pred_uncertainity(y_test, pred_mean, radius, pred_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
