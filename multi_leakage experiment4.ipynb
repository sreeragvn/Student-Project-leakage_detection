{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN 4 outputs and with and without sorting of outputs\n",
    "model_path = 'saved_model/Multi_leak/experiment4/Mask/'\n",
    "project_name='Multi_leak_experiment4_Mask_withSwap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 - 1 output layer with 1 loss function - mse. and do hyper parameter tuning.\n",
    "from utils.data_preprocess import load_data\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras_tuner as kt\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from kerastuner import HyperModel, Hyperband\n",
    "from keras import backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_leakage, two_leakage = load_data()\n",
    "# two_leakage[\"leak_1\"] = 1\n",
    "# two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "# single_leakage[\"leak_1\"] = 1\n",
    "# single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "data['x2'] = data['x2'].replace(np.nan, 0)\n",
    "data['y2'] = data['y2'].replace(np.nan, 0)\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual', 'total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2']]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "\n",
    "# sort x1, y1 and x2, y2. coordinates with lowest x will take the position of x1\n",
    "def coords_swap(y1):\n",
    "    s = y1['x2'] < y1['x1']\n",
    "    y1.loc[s, ['x1','x2']] = y1.loc[s, ['x2','x1']].values\n",
    "    y1.loc[s, ['y1','y2']] = y1.loc[s, ['y2','y1']].values\n",
    "    return y1\n",
    "\n",
    "\n",
    "y1_data = [y_train, y_val, y_test]\n",
    "y1_data_types = ['y_train', 'y_val', 'y_test']\n",
    "for y1_data_types, y1 in zip(y1_data_types, y1_data):\n",
    "    y1_data_types = coords_swap(y1)\n",
    "\n",
    "y1_train = y_train[['x1', 'y1']]\n",
    "y1_test = y_test[['x1', 'y1']]\n",
    "y1_val = y_val[['x1', 'y1']]\n",
    "\n",
    "y2_train = y_train[['x2', 'y2']]\n",
    "y2_test = y_test[['x2', 'y2']]\n",
    "y2_val = y_val[['x2', 'y2']]\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "scaler_coords2 = StandardScaler()\n",
    "y2_train = scaler_coords2.fit_transform(y2_train)\n",
    "y2_test = scaler_coords2.transform(y2_test)\n",
    "y2_val = scaler_coords2.transform(y2_val)\n",
    "\n",
    "y2_train = pd.DataFrame(y2_train, columns=y2_columns)\n",
    "y2_test = pd.DataFrame(y2_test, columns=y2_columns)\n",
    "y2_val = pd.DataFrame(y2_val, columns=y2_columns)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_columns)\n",
    "\n",
    "# y_train = [y1_train, y2_train]\n",
    "# y_val = [y1_val, y2_val]\n",
    "# y_test = [y1_test, y2_test]\n",
    "\n",
    "y_train_all = pd.concat([y1_train, y2_train], axis=1)\n",
    "y_test_all = pd.concat([y1_test, y2_test], axis=1)\n",
    "y_val_all = pd.concat([y1_val, y2_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, y1_train_np, y2_train_np = X_train.values, y1_train.values, y2_train.values\n",
    "X_val_np, y1_val_np, y2_val_np = X_val.values, y1_val.values, y2_val.values\n",
    "X_test_np, y1_test_np, y2_test_np = X_test.values, y1_test.values, y2_test.values\n",
    "\n",
    "# Create TensorFlow datasets from NumPy arrays.\n",
    "batch_size = 32\n",
    "buffer_size = len(X_train)  # Set the buffer size to the number of training examples for full shuffling.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y1_train_np, y2_train_np ))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y1_val_np, y2_val_np))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_np, y1_test_np, y2_test_np))\n",
    "\n",
    "# Shuffle, batch, and prefetch the training dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch the validation and test datasets.\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    mask = keras.backend.cast(keras.backend.not_equal(y_true, 0), keras.backend.floatx())\n",
    "    mse = keras.backend.mean(keras.backend.square(y_true - y_pred) * mask)\n",
    "    return mse\n",
    "\n",
    "losses = {\n",
    "\t\"y1\": \"mse\",\n",
    "\t\"y2\": masked_mse\n",
    "    # \"y2\" : 'mse'\n",
    "    }\n",
    "\n",
    "metrics = {\n",
    "    \"y1\": 'mae',\n",
    "    \"y2\": 'mae'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 Complete [00h 00m 07s]\n",
      "val_loss: 0.27254274909196285\n",
      "\n",
      "Best val_loss So Far: 0.27254274909196285\n",
      "Total elapsed time: 00h 01m 27s\n",
      "\n",
      "Search: Running Trial #12\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "13                |8                 |num_layers\n",
      "64                |512               |units_0\n",
      "9                 |7                 |task_0_num_layers\n",
      "0.017146          |0.0032825         |lr\n",
      "96                |160               |units_1\n",
      "288               |256               |units_2\n",
      "1                 |8                 |task_2_num_layers\n",
      "64                |64                |units_3\n",
      "288               |96                |units_4\n",
      "480               |128               |units_5\n",
      "480               |32                |units_6\n",
      "512               |192               |units_7\n",
      "224               |352               |units_8\n",
      "1                 |9                 |task_8_num_layers\n",
      "64                |24                |task_2_layer_0_neurons\n",
      "64                |32                |task_2_layer_1_neurons\n",
      "120               |72                |task_2_layer_2_neurons\n",
      "16                |120               |task_2_layer_3_neurons\n",
      "88                |112               |task_2_layer_4_neurons\n",
      "40                |112               |task_2_layer_5_neurons\n",
      "120               |112               |task_2_layer_6_neurons\n",
      "72                |80                |task_2_layer_7_neurons\n",
      "40                |128               |task_2_layer_8_neurons\n",
      "16                |8                 |task_2_layer_9_neurons\n",
      "160               |384               |units_9\n",
      "128               |256               |units_10\n",
      "0                 |0                 |task_10_num_layers\n",
      "1                 |10                |task_3_num_layers\n",
      "88                |72                |task_0_layer_0_neurons\n",
      "224               |64                |units_11\n",
      "448               |384               |units_12\n",
      "3                 |5                 |task_12_num_layers\n",
      "352               |64                |units_13\n",
      "384               |320               |units_14\n",
      "7                 |6                 |task_14_num_layers\n",
      "3                 |0                 |task_7_num_layers\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "10                |10                |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "# Define custom loss function\n",
    "\n",
    "\n",
    "# Define model builder function for Keras Tuner\n",
    "def model_builder(hp):\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Define model\n",
    "    inputs = keras.Input(shape=(10,))\n",
    "    shared_layer = inputs\n",
    "    for i in range(hp.Int('num_layers', 1, 15)):\n",
    "        shared_layer = layers.Dense(\n",
    "            units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "            activation='relu',\n",
    "            # add elu\n",
    "            kernel_initializer='he_uniform'\n",
    "        )(shared_layer)\n",
    "\n",
    "    out1 = shared_layer\n",
    "    out1 = layers.Dense(2, name='y1', activation = 'linear', kernel_initializer='he_uniform')(out1)\n",
    "\n",
    "    out2 = shared_layer\n",
    "    out2 = layers.Dense(2, name='y2', activation = 'linear', kernel_initializer='he_uniform')(out2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[out1, out2])\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "    # model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=['mse', masked_mse])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=hp_learning_rate),\n",
    "                    loss=losses, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    model_builder,\n",
    "    objective =  kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_epochs=EPOCHS+100,\n",
    "    factor=2,\n",
    "    directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "    project_name=project_name,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train, \n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), \n",
    "             verbose = 1, callbacks=[stop_early],\n",
    "             epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hps)\n",
    "\n",
    "# Build the best model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "history = best_model.fit(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train,\n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),  \n",
    "            callbacks=[stop_early],\n",
    "            verbose = 1, epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
    "    is {best_model.optimizer.lr.numpy()}.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(model_path)\n",
    "best_model = tf.keras.models.load_model(model_path)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions_train = best_model.predict(train_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "# print(\"train\", \"{:10.4f}\".format(mean_squared_error(y_train, y_predictions, squared=True)))\n",
    "y_predictions_val = best_model.predict(val_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "# print(\"val\", \"{:10.4f}\".format(mean_squared_error(y_val, y_predictions, squared=True)))\n",
    "y_predictions = best_model.predict(test_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "\n",
    "\n",
    "loss_test = \"{:10.4f}\".format(mean_squared_error(y_test_all, y_predictions, squared=True))\n",
    "metric_test = \"{:10.4f}\".format(mean_absolute_error(y_test_all, y_predictions))\n",
    "\n",
    "loss_val = \"{:10.4f}\".format(mean_squared_error(y_val_all, y_predictions_val, squared=True))\n",
    "metric_val = \"{:10.4f}\".format(mean_absolute_error(y_val_all, y_predictions_val))\n",
    "\n",
    "loss_train = \"{:10.4f}\".format(mean_squared_error(y_train_all, y_predictions_train, squared=True))\n",
    "metric_train = \"{:10.4f}\".format(mean_absolute_error(y_train_all, y_predictions_train))\n",
    "\n",
    "print(metric_test, metric_val, metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = best_model.evaluate(train_dataset.map(lambda x, y1: (x, {'y1': y1})), verbose=1)\n",
    "results_val = best_model.evaluate(val_dataset.map(lambda x, y1: (x, {'y1': y1})), verbose=1)\n",
    "results_test = best_model.evaluate(test_dataset.map(lambda x, y1: (x, {'y1': y1,})), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
