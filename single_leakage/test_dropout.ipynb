{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import tensorflow as tf\n",
    "from utils.data_preprocess import load_data\n",
    "from utils.module import model_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from utils.model_evaluation import plot_test_pred\n",
    "\n",
    "# Sandwiched dropout layer to the hyperparameter tuned model and did the predictions 1000 times without training the model again\n",
    "# Sandwiched dropout layer to the hyperparameter tuned model and did the predictions 1000 times after training the model again\n",
    "# During hyperparameter tuning, it is ensured that a dropout layer is there after a dense layer. This model is directly used for uncertainity quanitification\n",
    "\n",
    "single_leakage, two_leakage = load_data()\n",
    "# print(single_leakage.columns)\n",
    "data = single_leakage.drop(columns=['total flow rate', 'mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual'])\n",
    "\n",
    "print(data.columns)\n",
    "print(data.shape)\n",
    "\n",
    "y = data[['x1', 'y1']]\n",
    "x = data.drop(['x1', 'y1'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "scaler_coords = StandardScaler()\n",
    "y_train = scaler_coords.fit_transform(y_train)\n",
    "y_test = scaler_coords.transform(y_test)\n",
    "y_val = scaler_coords.transform(y_val)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)\n",
    "\n",
    "# %%\n",
    "dropout_prob = 0.1\n",
    "\n",
    "model = tf.keras.models.load_model('saved_model/single_leak/single_leakage_model_less')\n",
    "print(model.summary())\n",
    "model_evaluate, y_pred = model_eval(model, X_test, y_test, X_train, y_train, X_val, y_val)\n",
    "\n",
    "stoch_model = tf.keras.Sequential()\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    stoch_model.add(layer)\n",
    "    # Add your intermediate layer after each existing layer\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if i == len(model.layers)-1:\n",
    "        continue\n",
    "    intermediate_layer = tf.keras.layers.Dropout(dropout_prob)\n",
    "    stoch_model.add(intermediate_layer)\n",
    "\n",
    "# Compile the new model\n",
    "stoch_model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=model.optimizer.lr.numpy()),\n",
    "                loss=\"mse\",\n",
    "                metrics='mae')\n",
    "# Print the summary of the new model\n",
    "stoch_model.summary()\n",
    "model_evaluate, y_pred = model_eval(stoch_model, X_test, y_test, X_train, y_train, X_val, y_val)\n",
    "# # # %%\n",
    "pred=np.stack([stoch_model(X_test,training=True) \n",
    "               for sample in range(1000)])\n",
    "predictions_list = pred.tolist()\n",
    "predictions_list_unsc = []\n",
    "# print(len(predictions_list))\n",
    "for pred in predictions_list:\n",
    "    pred = scaler_coords.inverse_transform(pred)\n",
    "    predictions_list_unsc.append(pred)\n",
    "predictions__unsc = np.array(predictions_list_unsc)\n",
    "\n",
    "# print(predictions__unsc.shape)\n",
    "pred_mean=predictions__unsc.mean(axis=0)\n",
    "pred_std = predictions__unsc.std(axis=0) \n",
    "# print(pred_mean.shape, pred_std.shape)\n",
    "# # %%\n",
    "y_test = scaler_coords.inverse_transform(y_test)\n",
    "plot_test_pred(y_test, pred_mean)\n",
    "\n",
    "\n",
    "# # # %%\n",
    "# pred_mean_un = scaler_coords.inverse_transform(pred_mean)\n",
    "# pred_std_un = scaler_coords.inverse_transform(pred_std)\n",
    "# should we do a \n",
    "# radius = np.sqrt((pred_mean_un.transpose()[0] - pred_std_un.transpose()[0])**2 + \n",
    "#                  (pred_mean_un.transpose()[1] - pred_std_un.transpose()[1])**2)\n",
    "\n",
    "# # %%\n",
    "# print('mean x coords')\n",
    "# print(pred_mean_un.transpose()[0])\n",
    "\n",
    "# # # %%\n",
    "# print('std x coords')\n",
    "# print(pred_std_un.transpose()[0])\n",
    "\n",
    "# # # %%\n",
    "# print('mean y coords')\n",
    "# print(pred_mean_un.transpose()[1])\n",
    "\n",
    "# # # %%\n",
    "# print('std y coords')\n",
    "# print(pred_std_un.transpose()[1])\n",
    "\n",
    "# # # %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
