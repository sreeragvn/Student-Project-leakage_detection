{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN 6 outputs and status of swapping has to be decided on the basis of experiment 2\n",
    "model_path = 'saved_model/Multi_leak/experiment3/withswap/sameloss/'\n",
    "project_name='Multi_leak_experiment3_MTL_withSwap_sameloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.1\n",
      "# GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 - 1 output layer with 1 loss function - mse. and do hyper parameter tuning.\n",
    "from utils.data_preprocess import load_data, load_single_leakage_model_data\n",
    "from utils.module import model_eval, hyper_model, model_comparison, linear_regression, numpy_to_tensor, benchmark_linear_model\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from kerastuner import HyperModel, Hyperband\n",
    "from keras import backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_leakage, two_leakage = load_data()\n",
    "# two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "# single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "# use the idea of masking instead\n",
    "data['x2'] = data['x2'].replace(np.nan, 8024)\n",
    "data['y2'] = data['y2'].replace(np.nan, 2616.5)\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual','total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', 'leak_2']]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', 'leak_2'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "\n",
    "# sort x1, y1 and x2, y2. coordinates with lowest x will take the position of x1\n",
    "def coords_swap(y1):\n",
    "    s = y1['x2'] < y1['x1']\n",
    "    y1.loc[s, ['x1','x2']] = y1.loc[s, ['x2','x1']].values\n",
    "    y1.loc[s, ['y1','y2']] = y1.loc[s, ['y2','y1']].values\n",
    "    return y1\n",
    "\n",
    "y1_train = y_train[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_train = y_train[[ 'leak_2']]\n",
    "y1_test = y_test[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_test = y_test[[ 'leak_2']]\n",
    "y1_val = y_val[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_val = y_val[[ 'leak_2']]\n",
    "\n",
    "\n",
    "# y1_test.to_csv('check1.csv')\n",
    "\n",
    "y1_data = [y1_train, y1_val, y1_test]\n",
    "y1_data_types = ['y1_train', 'y1_val', 'y1_test']\n",
    "for y1_data_types, y1 in zip(y1_data_types, y1_data):\n",
    "    y1_data_types = coords_swap(y1)\n",
    "\n",
    "# print()\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "# y1_test.to_csv('check2.csv')\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "y2_train = y2_train.reset_index().drop(columns='sample_number')\n",
    "y2_val = y2_val.reset_index().drop(columns='sample_number')\n",
    "y2_test = y2_test.reset_index().drop(columns='sample_number')\n",
    "\n",
    "y_train_all = pd.concat([y1_train, y2_train], axis=1)\n",
    "y_test_all = pd.concat([y1_test, y2_test], axis=1)\n",
    "y_val_all = pd.concat([y1_val, y2_val], axis=1)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, y1_train_np, y2_train_np = X_train.values, y1_train.values, y2_train.values\n",
    "X_val_np, y1_val_np, y2_val_np = X_val.values, y1_val.values, y2_val.values\n",
    "X_test_np, y1_test_np, y2_test_np = X_test.values, y1_test.values, y2_test.values\n",
    "\n",
    "# Create TensorFlow datasets from NumPy arrays.\n",
    "batch_size = 32\n",
    "buffer_size = len(X_train)  # Set the buffer size to the number of training examples for full shuffling.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y1_train_np, y2_train_np ))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y1_val_np, y2_val_np))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_np, y1_test_np, y2_test_np))\n",
    "\n",
    "# Shuffle, batch, and prefetch the training dataset.\n",
    "train_dataset_org = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch the validation and test datasets.\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = {\n",
    "#     \"y1\" : y1_train,\n",
    "#     \"y2\" : y2_train\n",
    "# }\n",
    "\n",
    "# y_val = {\n",
    "#     \"y1\" : y1_val,\n",
    "#     \"y2\" : y2_val\n",
    "# }\n",
    "\n",
    "# y_test = {\n",
    "#     \"y1\" : y1_test,\n",
    "#     \"y2\" : y2_test\n",
    "# }\n",
    "\n",
    "losses = {\n",
    "\t\"y1\": \"mse\",\n",
    "\t\"y2\": 'mse'\n",
    "    # \"y2\" : 'mse'\n",
    "    }\n",
    "\n",
    "metrics = {\n",
    "    \"y1\": 'mae',\n",
    "    \"y2\": 'mae'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 613 Complete [00h 00m 13s]\n",
      "val_loss: 0.8743064401430658\n",
      "\n",
      "Best val_loss So Far: 0.0936473066353321\n",
      "Total elapsed time: 00h 12m 52s\n",
      "\n",
      "Search: Running Trial #614\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "14                |3                 |num_layers\n",
      "288               |480               |units_0\n",
      "10                |5                 |task_0_num_layers\n",
      "0.0007071         |0.002596          |lr\n",
      "96                |480               |units_1\n",
      "160               |288               |units_2\n",
      "320               |128               |units_3\n",
      "256               |288               |units_4\n",
      "384               |128               |units_5\n",
      "32                |480               |units_6\n",
      "224               |448               |units_7\n",
      "320               |256               |units_8\n",
      "288               |448               |units_9\n",
      "320               |320               |units_10\n",
      "128               |320               |units_11\n",
      "10                |5                 |task_11_num_layers\n",
      "6                 |1                 |task_8_num_layers\n",
      "2                 |8                 |task_3_num_layers\n",
      "64                |384               |units_12\n",
      "32                |32                |units_13\n",
      "64                |128               |units_14\n",
      "7                 |1                 |task_14_num_layers\n",
      "1                 |4                 |task_12_num_layers\n",
      "9                 |2                 |task_7_num_layers\n",
      "56                |104               |task_12_layer_0_neurons\n",
      "48                |120               |task_12_layer_1_neurons\n",
      "24                |104               |task_12_layer_2_neurons\n",
      "96                |56                |task_12_layer_3_neurons\n",
      "8                 |96                |task_12_layer_4_neurons\n",
      "24                |32                |task_0_layer_0_neurons\n",
      "32                |40                |task_0_layer_1_neurons\n",
      "88                |56                |task_0_layer_2_neurons\n",
      "128               |128               |task_0_layer_3_neurons\n",
      "48                |8                 |task_0_layer_4_neurons\n",
      "72                |16                |task_0_layer_5_neurons\n",
      "24                |24                |task_0_layer_6_neurons\n",
      "120               |80                |task_0_layer_7_neurons\n",
      "72                |16                |task_0_layer_8_neurons\n",
      "88                |40                |task_0_layer_9_neurons\n",
      "7                 |8                 |task_1_num_layers\n",
      "4                 |8                 |task_10_num_layers\n",
      "72                |80                |task_14_layer_0_neurons\n",
      "88                |112               |task_3_layer_0_neurons\n",
      "104               |16                |task_3_layer_1_neurons\n",
      "120               |16                |task_3_layer_2_neurons\n",
      "40                |96                |task_3_layer_3_neurons\n",
      "112               |80                |task_3_layer_4_neurons\n",
      "64                |96                |task_3_layer_5_neurons\n",
      "24                |72                |task_3_layer_6_neurons\n",
      "5                 |3                 |task_6_num_layers\n",
      "5                 |2                 |task_9_num_layers\n",
      "16                |64                |task_12_layer_5_neurons\n",
      "56                |72                |task_12_layer_6_neurons\n",
      "96                |112               |task_8_layer_0_neurons\n",
      "96                |24                |task_8_layer_1_neurons\n",
      "128               |80                |task_8_layer_2_neurons\n",
      "8                 |8                 |task_8_layer_3_neurons\n",
      "32                |40                |task_8_layer_4_neurons\n",
      "88                |8                 |task_8_layer_5_neurons\n",
      "40                |24                |task_8_layer_6_neurons\n",
      "4                 |0                 |task_5_num_layers\n",
      "64                |16                |task_7_layer_0_neurons\n",
      "16                |88                |task_7_layer_1_neurons\n",
      "104               |16                |task_7_layer_2_neurons\n",
      "32                |32                |task_7_layer_3_neurons\n",
      "88                |72                |task_12_layer_7_neurons\n",
      "56                |32                |task_11_layer_0_neurons\n",
      "16                |128               |task_11_layer_1_neurons\n",
      "112               |128               |task_11_layer_2_neurons\n",
      "32                |72                |task_11_layer_3_neurons\n",
      "40                |88                |task_11_layer_4_neurons\n",
      "128               |80                |task_11_layer_5_neurons\n",
      "8                 |80                |task_11_layer_6_neurons\n",
      "128               |120               |task_11_layer_7_neurons\n",
      "72                |56                |task_11_layer_8_neurons\n",
      "64                |32                |task_11_layer_9_neurons\n",
      "128               |8                 |task_9_layer_0_neurons\n",
      "72                |64                |task_10_layer_0_neurons\n",
      "88                |88                |task_10_layer_1_neurons\n",
      "32                |96                |task_10_layer_2_neurons\n",
      "128               |88                |task_10_layer_3_neurons\n",
      "8                 |32                |task_10_layer_4_neurons\n",
      "16                |8                 |task_10_layer_5_neurons\n",
      "56                |8                 |task_10_layer_6_neurons\n",
      "72                |96                |task_5_layer_0_neurons\n",
      "48                |120               |task_5_layer_1_neurons\n",
      "120               |64                |task_5_layer_2_neurons\n",
      "72                |88                |task_1_layer_0_neurons\n",
      "16                |40                |task_1_layer_1_neurons\n",
      "112               |56                |task_1_layer_2_neurons\n",
      "24                |16                |task_1_layer_3_neurons\n",
      "120               |48                |task_1_layer_4_neurons\n",
      "8                 |32                |task_1_layer_5_neurons\n",
      "80                |48                |task_1_layer_6_neurons\n",
      "120               |128               |task_1_layer_7_neurons\n",
      "24                |96                |task_1_layer_8_neurons\n",
      "80                |40                |task_1_layer_9_neurons\n",
      "72                |8                 |task_7_layer_4_neurons\n",
      "128               |80                |task_7_layer_5_neurons\n",
      "24                |8                 |task_7_layer_6_neurons\n",
      "120               |120               |task_7_layer_7_neurons\n",
      "128               |120               |task_7_layer_8_neurons\n",
      "8                 |104               |task_7_layer_9_neurons\n",
      "6                 |1                 |task_2_num_layers\n",
      "24                |104               |task_2_layer_0_neurons\n",
      "32                |32                |task_2_layer_1_neurons\n",
      "104               |24                |task_2_layer_2_neurons\n",
      "24                |72                |task_2_layer_3_neurons\n",
      "64                |128               |task_9_layer_1_neurons\n",
      "96                |40                |task_9_layer_2_neurons\n",
      "72                |48                |task_9_layer_3_neurons\n",
      "24                |96                |task_9_layer_4_neurons\n",
      "32                |24                |task_9_layer_5_neurons\n",
      "120               |80                |task_9_layer_6_neurons\n",
      "120               |112               |task_9_layer_7_neurons\n",
      "1                 |9                 |task_13_num_layers\n",
      "8                 |80                |task_12_layer_8_neurons\n",
      "40                |8                 |task_6_layer_0_neurons\n",
      "32                |16                |task_6_layer_1_neurons\n",
      "24                |40                |task_2_layer_4_neurons\n",
      "56                |112               |task_2_layer_5_neurons\n",
      "96                |120               |task_2_layer_6_neurons\n",
      "8                 |7                 |task_4_num_layers\n",
      "112               |112               |task_13_layer_0_neurons\n",
      "72                |48                |task_13_layer_1_neurons\n",
      "48                |112               |task_13_layer_2_neurons\n",
      "40                |8                 |task_13_layer_3_neurons\n",
      "48                |104               |task_13_layer_4_neurons\n",
      "16                |96                |task_13_layer_5_neurons\n",
      "72                |96                |task_13_layer_6_neurons\n",
      "40                |72                |task_13_layer_7_neurons\n",
      "120               |88                |task_13_layer_8_neurons\n",
      "32                |80                |task_13_layer_9_neurons\n",
      "112               |88                |task_6_layer_2_neurons\n",
      "120               |72                |task_6_layer_3_neurons\n",
      "96                |120               |task_6_layer_4_neurons\n",
      "40                |40                |task_6_layer_5_neurons\n",
      "48                |16                |task_6_layer_6_neurons\n",
      "128               |128               |task_6_layer_7_neurons\n",
      "64                |104               |task_14_layer_1_neurons\n",
      "56                |40                |task_14_layer_2_neurons\n",
      "16                |56                |task_5_layer_3_neurons\n",
      "104               |120               |task_5_layer_4_neurons\n",
      "104               |56                |task_5_layer_5_neurons\n",
      "112               |120               |task_5_layer_6_neurons\n",
      "88                |80                |task_5_layer_7_neurons\n",
      "72                |16                |task_5_layer_8_neurons\n",
      "80                |128               |task_5_layer_9_neurons\n",
      "112               |112               |task_3_layer_7_neurons\n",
      "120               |8                 |task_3_layer_8_neurons\n",
      "24                |8                 |task_4_layer_0_neurons\n",
      "16                |48                |task_4_layer_1_neurons\n",
      "72                |48                |task_4_layer_2_neurons\n",
      "56                |40                |task_4_layer_3_neurons\n",
      "48                |128               |task_4_layer_4_neurons\n",
      "8                 |32                |task_4_layer_5_neurons\n",
      "24                |128               |task_4_layer_6_neurons\n",
      "16                |88                |task_4_layer_7_neurons\n",
      "8                 |56                |task_4_layer_8_neurons\n",
      "32                |40                |task_4_layer_9_neurons\n",
      "128               |88                |task_10_layer_7_neurons\n",
      "120               |16                |task_9_layer_8_neurons\n",
      "48                |88                |task_10_layer_8_neurons\n",
      "16                |32                |task_10_layer_9_neurons\n",
      "112               |24                |task_8_layer_7_neurons\n",
      "72                |48                |task_8_layer_8_neurons\n",
      "96                |72                |task_14_layer_3_neurons\n",
      "128               |112               |task_14_layer_4_neurons\n",
      "104               |40                |task_14_layer_5_neurons\n",
      "88                |128               |task_14_layer_6_neurons\n",
      "96                |8                 |task_14_layer_7_neurons\n",
      "112               |32                |task_3_layer_9_neurons\n",
      "88                |56                |task_14_layer_8_neurons\n",
      "128               |120               |task_2_layer_7_neurons\n",
      "64                |96                |task_2_layer_8_neurons\n",
      "120               |16                |task_6_layer_8_neurons\n",
      "72                |120               |task_6_layer_9_neurons\n",
      "48                |128               |task_14_layer_9_neurons\n",
      "24                |8                 |task_2_layer_9_neurons\n",
      "32                |96                |task_8_layer_9_neurons\n",
      "72                |88                |task_9_layer_9_neurons\n",
      "88                |72                |task_12_layer_9_neurons\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "10                |10                |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 with Nadam\n",
    "\n",
    "EPOCHS = 1000\n",
    "# Define the multi-task HyperModel\n",
    "class MultiTaskHyperModel(HyperModel):\n",
    "\n",
    "    def build(self, hp):\n",
    "        backend.clear_session()\n",
    "        inputs = keras.Input(shape=(10,))\n",
    "        shared_layer = inputs\n",
    "        for i in range(hp.Int('num_layers', 1, 15)):\n",
    "            shared_layer = layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "                # activation=hp.Choice(\"activation\", [\"relu\", 'elu']),\n",
    "                activation = 'relu',\n",
    "                # add elu\n",
    "                kernel_initializer='he_uniform'\n",
    "            )(shared_layer)\n",
    "\n",
    "\n",
    "        task_layer1 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "            task_layer1 = layers.Dense(units=hp.Int(f'task_{i}_layer_{j}_neurons', min_value=8, max_value=128, step=8), \n",
    "                                       activation='relu',\n",
    "                                       kernel_initializer='he_uniform')(task_layer1)\n",
    "        y1 = layers.Dense(4, name='y1', activation = 'linear', kernel_initializer='he_uniform')(task_layer1)\n",
    "\n",
    "        task_layer2 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "            task_layer2 = layers.Dense(units=hp.Int(f'task_{i}_layer_{j}_neurons', min_value=2, max_value=64, step=2),\n",
    "                                        activation='relu',\n",
    "                                        kernel_initializer='he_uniform')(task_layer2)\n",
    "        y2 = layers.Dense(1, name='y2', activation = 'sigmoid', kernel_initializer='he_uniform')(task_layer2)\n",
    "\n",
    "        outputs = [y1, y2]\n",
    "\n",
    "        # loss1_weight = hp.Float(\"loss1_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "        # loss1_weight = hp.Float(\"loss2_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "        \n",
    "        # loss1_weight = hp.Choice('loss1_weight', values=[0.6, 0.7, 0.8, 0.9])\n",
    "        # loss2_weight = hp.Choice('loss2_weight', values=[0.00005, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        # to stop the model from reducing both these weightage to very low and hence reducing the total loss\n",
    "        # loss2_weight = 1 - loss1_weight\n",
    "\n",
    "        lossWeights = {\"y1\": 0.9, \"y2\": 0.1}\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "        # model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        #             loss=\"mse\",  metrics='mae')\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                    loss=losses, \n",
    "                    loss_weights=lossWeights,\n",
    "                    metrics = metrics)\n",
    "        return model\n",
    "\n",
    "# Create the multi-task HyperModel\n",
    "multi_task_hypermodel = MultiTaskHyperModel()\n",
    "\n",
    "# Define the Hyperband tuner\n",
    "tuner = Hyperband(\n",
    "    multi_task_hypermodel,\n",
    "    objective =  kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_epochs=EPOCHS+100,\n",
    "    factor=2,\n",
    "    directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "    project_name=project_name,\n",
    "    seed=0,   \n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train, \n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), \n",
    "             verbose = 1, callbacks=[stop_early],\n",
    "             epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hps)\n",
    "\n",
    "# Build the best model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "history = best_model.fit(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train, \n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),  \n",
    "            callbacks=[stop_early],\n",
    "            verbose = 1, epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
    "    is {best_model.optimizer.lr.numpy()}.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save(model_path)\n",
    "best_model = tf.keras.models.load_model(model_path)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_predictions_train = best_model.predict(train_dataset_org.map(lambda x, y1: (x, {'y1': y1})))\n",
    "# print(\"train\", \"{:10.4f}\".format(mean_squared_error(y_train, y_predictions, squared=True)))\n",
    "y_predictions_val = best_model.predict(val_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "# print(\"val\", \"{:10.4f}\".format(mean_squared_error(y_val, y_predictions, squared=True)))\n",
    "y_predictions = best_model.predict(test_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "\n",
    "y_predictions_train[:,-2][np.abs(y_predictions_train[:,-2]) < 0.5] = 0\n",
    "y_predictions_train[:,-2][np.abs(y_predictions_train[:,-2]) > 0.5] = 1\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) < 0.5] = 0\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions_val[:,-2][np.abs(y_predictions_val[:,-2]) < 0.5] = 0\n",
    "y_predictions_val[:,-2][np.abs(y_predictions_val[:,-2]) > 0.5] = 1\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) < 0.5] = 0\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions[:,-2][np.abs(y_predictions[:,-2]) < 0.5] = 0\n",
    "y_predictions[:,-2][np.abs(y_predictions[:,-2]) > 0.5] = 1\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) < 0.5] = 0\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) > 0.5] = 1\n",
    "\n",
    "loss_test = \"{:10.4f}\".format(mean_squared_error(y_test_all, y_predictions, squared=True))\n",
    "metric_test = \"{:10.4f}\".format(mean_absolute_error(y_test_all, y_predictions))\n",
    "\n",
    "loss_val = \"{:10.4f}\".format(mean_squared_error(y_val_all, y_predictions_val, squared=True))\n",
    "metric_val = \"{:10.4f}\".format(mean_absolute_error(y_val_all, y_predictions_val))\n",
    "\n",
    "loss_train = \"{:10.4f}\".format(mean_squared_error(y_train_all, y_predictions_train, squared=True))\n",
    "metric_train = \"{:10.4f}\".format(mean_absolute_error(y_train_all, y_predictions_train))\n",
    "\n",
    "print(metric_test, metric_val, metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = best_model.evaluate(train_dataset.map(lambda x, y1: (x, {'y1': y1})), verbose=1)\n",
    "results_val = best_model.evaluate(val_dataset.map(lambda x, y1: (x, {'y1': y1})), verbose=1)\n",
    "results_test = best_model.evaluate(test_dataset.map(lambda x, y1: (x, {'y1': y1,})), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
