{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 - 1 output layer with 1 loss function - mse. and do hyper parameter tuning.\n",
    "from utils.data_preprocess import load_data, load_single_leakage_model_data\n",
    "from utils.module import model_eval, hyper_model, model_comparison, linear_regression, numpy_to_tensor, benchmark_linear_model\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_model/Multi_leak/1_loss_linear_reg/'\n",
    "project_name='multileak_6out_1loss_withswap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x1', 'y1', 'MFC6', 'MFC7', 'MFC8', 'MFC9', 'MFC10', 'MFC1', 'MFC2',\n",
      "       'MFC3', 'MFC4', 'MFC5', 'leak_1', 'leak_2', 'x2', 'y2'],\n",
      "      dtype='object')\n",
      "x1        0\n",
      "y1        0\n",
      "MFC6      0\n",
      "MFC7      0\n",
      "MFC8      0\n",
      "MFC9      0\n",
      "MFC10     0\n",
      "MFC1      0\n",
      "MFC2      0\n",
      "MFC3      0\n",
      "MFC4      0\n",
      "MFC5      0\n",
      "leak_1    0\n",
      "leak_2    0\n",
      "x2        0\n",
      "y2        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open(\"config_multi.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.full_load(ymlfile)\n",
    "\n",
    "\n",
    "single_leakage, two_leakage = load_data()\n",
    "two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "data['x2'] = data['x2'].replace(np.nan, 8024)\n",
    "data['y2'] = data['y2'].replace(np.nan, 2616.5)\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual', 'total flow rate'])\n",
    "\n",
    "print(data.columns)\n",
    "print(data.isna().sum())\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2']]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "y1_train = y_train[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_train = y_train[['leak_1', 'leak_2']]\n",
    "y1_test = y_test[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_test = y_test[['leak_1', 'leak_2']]\n",
    "y1_val = y_val[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_val = y_val[['leak_1', 'leak_2']]\n",
    "\n",
    "def coords_swap(y1):\n",
    "    s = y1['x2'] < y1['x1']\n",
    "    y1.loc[s, ['x1','x2']] = y1.loc[s, ['x2','x1']].values\n",
    "    y1.loc[s, ['y1','y2']] = y1.loc[s, ['y2','y1']].values\n",
    "    return y1\n",
    "\n",
    "y1_data = [y1_train, y1_val, y1_test]\n",
    "y1_data_types = ['y1_train', 'y1_val', 'y1_test']\n",
    "for y1_data_types, y1 in zip(y1_data_types, y1_data):\n",
    "    y1_data_types = coords_swap(y1)\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "# y1_train['x2'] = y1_train['x2'].replace(np.nan, -5)\n",
    "# y1_train['y2'] = y1_train['y2'].replace(np.nan, -5)\n",
    "\n",
    "# y1_test['x2'] = y1_test['x2'].replace(np.nan, -5)\n",
    "# y1_test['y2'] = y1_test['y2'].replace(np.nan, -5)\n",
    "\n",
    "# y1_val['x2'] = y1_val['x2'].replace(np.nan, -5)\n",
    "# y1_val['y2'] = y1_val['y2'].replace(np.nan, -5)\n",
    "# Not sure if 0 is good enough or try generating a random number\n",
    "\n",
    "# scaler_coords2 = StandardScaler()\n",
    "# y2_train = scaler_coords2.fit_transform(y2_train)\n",
    "# y2_test = scaler_coords2.fit_transform(y2_test)\n",
    "# y2_val = scaler_coords2.transform(y2_val)\n",
    "\n",
    "# y2_train = pd.DataFrame(y2_train, columns=y2_columns)\n",
    "# y2_test = pd.DataFrame(y2_test, columns=y2_columns)\n",
    "# y2_val = pd.DataFrame(y2_val, columns=y2_columns)\n",
    "\n",
    "y2_train = y2_train.reset_index().drop(columns='sample_number')\n",
    "y2_val = y2_val.reset_index().drop(columns='sample_number')\n",
    "y2_test = y2_test.reset_index().drop(columns='sample_number')\n",
    "\n",
    "y_train_sc = pd.concat([y1_train, y2_train], axis=1)\n",
    "y_test_sc = pd.concat([y1_test, y2_test], axis=1)\n",
    "y_val_sc = pd.concat([y1_val, y2_val], axis=1)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple linear regression model\n",
    "reg = LinearRegression().fit(X_train, y_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.2344     0.2108     0.2260\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_predictions_train = reg.predict(X_train)\n",
    "# print(\"train\", \"{:10.4f}\".format(mean_squared_error(y_train, y_predictions, squared=True)))\n",
    "y_predictions_val = reg.predict(X_val)\n",
    "# print(\"val\", \"{:10.4f}\".format(mean_squared_error(y_val, y_predictions, squared=True)))\n",
    "y_predictions = reg.predict(X_test)\n",
    "\n",
    "y_predictions_train[:,-2][np.abs(y_predictions_train[:,-2]) < 0.5] = 0\n",
    "y_predictions_train[:,-2][np.abs(y_predictions_train[:,-2]) > 0.5] = 1\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) < 0.5] = 0\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions_val[:,-2][np.abs(y_predictions_val[:,-2]) < 0.5] = 0\n",
    "y_predictions_val[:,-2][np.abs(y_predictions_val[:,-2]) > 0.5] = 1\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) < 0.5] = 0\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions[:,-2][np.abs(y_predictions[:,-2]) < 0.5] = 0\n",
    "y_predictions[:,-2][np.abs(y_predictions[:,-2]) > 0.5] = 1\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) < 0.5] = 0\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) > 0.5] = 1\n",
    "\n",
    "loss_test = \"{:10.4f}\".format(mean_squared_error(y_test_sc, y_predictions, squared=True))\n",
    "metric_test = \"{:10.4f}\".format(mean_absolute_error(y_test_sc, y_predictions))\n",
    "\n",
    "loss_val = \"{:10.4f}\".format(mean_squared_error(y_val_sc, y_predictions_val, squared=True))\n",
    "metric_val = \"{:10.4f}\".format(mean_absolute_error(y_val_sc, y_predictions_val))\n",
    "\n",
    "loss_train = \"{:10.4f}\".format(mean_squared_error(y_train_sc, y_predictions_train, squared=True))\n",
    "metric_train = \"{:10.4f}\".format(mean_absolute_error(y_train_sc, y_predictions_train))\n",
    "\n",
    "print(metric_test, metric_val, metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting the predictions to certainity of 0 and 1 (May not be a good idea)\n",
    "# y_pred[:,-2][np.abs(y_pred[:,-2]) < 0.5] = 0\n",
    "# y_pred[:,-2][np.abs(y_pred[:,-2]) > 0.5] = 1\n",
    "# y_pred[:,-1][np.abs(y_pred[:,-1]) < 0.5] = 0\n",
    "# y_pred[:,-1][np.abs(y_pred[:,-1]) > 0.5] = 1\n",
    "\n",
    "# y_pred = pd.DataFrame(y_pred, columns=y_train.columns)\n",
    "# y1_pred_inverse = scaler_coords1.inverse_transform(y_pred[['x1', 'y1','x2', 'y2']])\n",
    "# y_pred[['x1', 'y1','x2', 'y2']] = pd.DataFrame(y1_pred_inverse,columns=['x1', 'y1','x2', 'y2'])\n",
    "# mse = score_mse(y_test, y_pred)\n",
    "# y_pred.loc[y_pred['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "# y_pred.loc[y_pred['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "\n",
    "# y_test.loc[y_test['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "# y_test.loc[y_test['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "# pd.concat([y_pred, y_test.reset_index().drop(columns='sample_number')], axis=1).to_csv(model_path+'predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model using model builder from keras tuner\n",
    "def model_builder_single(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Choose an optimal value between 32-512\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 15)):\n",
    "        l1_weight = hp.Choice('l1_weight', values=[0.0, 1e-1, 1e-2, 1e-3])\n",
    "        l2_weight = hp.Choice('l2_weight', values=[0.0, 1e-1, 1e-2, 1e-3])\n",
    "        kernel_regularizer=keras.regularizers.L1L2(l1 = l1_weight, l2 = l2_weight)\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "                kernel_initializer='he_uniform',\n",
    "                kernel_regularizer=kernel_regularizer\n",
    "            )\n",
    "        )\n",
    "    model.add(keras.layers.Dense(units=6, activation= \"linear\", kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    # hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "    # model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    #             loss=\"mse\",  metrics='mae')\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                loss=\"mse\",\n",
    "                metrics='mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#search for the best hyperparameters and train the standard model with original training data\n",
    "def hyper_model(X_train,Y_train, X_val, y_val, epoch, factor):\n",
    "    folder_name = project_name\n",
    "    tuner = kt.Hyperband(model_builder_single,\n",
    "                         objective='val_loss',\n",
    "                         max_epochs=epoch+200,\n",
    "                         factor=factor,\n",
    "                         hyperband_iterations = 1,\n",
    "                        # Integer, at least 1, the number of times to iterate over the full Hyperband algorithm. One iteration will \n",
    "                        # run approximately max_epochs * (math.log(max_epochs, factor) ** 2) cumulative epochs across all trials. It is \n",
    "                        # recommended to set this to as high a value as is within your resource budget. Defaults to 1.\n",
    "                         directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "                         seed=0,    \n",
    "                         project_name=str(folder_name))\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train, Y_train, epochs=epoch, validation_data = (X_val, y_val), callbacks=[stop_early, \n",
    "                                                                                            #   keras.callbacks.TensorBoard(\"../tensorflow_log_files/studienarbeit/tb_logs\"+str(folder_name))\n",
    "                                                                                              ])\n",
    "    #tuner.search(X_train, Y_train, epochs=50, validation_data=(X_test,Y_test), callbacks=[stop_early])\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(X_train, Y_train, epochs=epoch, validation_data = (X_val, y_val), shuffle= True)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
    "    is {model.optimizer.lr.numpy()}.\n",
    "    \"\"\")\n",
    "\n",
    "    return best_hps, model, tuner, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from ../../tensorflow_log_files/studienarbeit/multileak_6out_1loss_withswap/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from ../../tensorflow_log_files/studienarbeit/multileak_6out_1loss_withswap/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 20\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 15, 'step': 1, 'sampling': 'linear'}\n",
      "l1_weight (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.1, 0.01, 0.001], 'ordered': True}\n",
      "l2_weight (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.1, 0.01, 0.001], 'ordered': True}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.1, 'step': None, 'sampling': 'log'}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_5 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_6 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_7 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_8 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_9 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_10 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_11 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_12 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_13 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "units_14 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 3s 20ms/step - loss: 0.5086 - mae: 0.5344 - val_loss: 0.1895 - val_mae: 0.3336\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1298 - mae: 0.2637 - val_loss: 0.1217 - val_mae: 0.2556\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0765 - mae: 0.1979 - val_loss: 0.0891 - val_mae: 0.2124\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0571 - mae: 0.1671 - val_loss: 0.0807 - val_mae: 0.1920\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0452 - mae: 0.1457 - val_loss: 0.0722 - val_mae: 0.1855\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0383 - mae: 0.1344 - val_loss: 0.0701 - val_mae: 0.1833\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0333 - mae: 0.1236 - val_loss: 0.0669 - val_mae: 0.1718\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0298 - mae: 0.1175 - val_loss: 0.0643 - val_mae: 0.1665\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0281 - mae: 0.1126 - val_loss: 0.0670 - val_mae: 0.1669\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0277 - mae: 0.1119 - val_loss: 0.0629 - val_mae: 0.1649\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0231 - mae: 0.1020 - val_loss: 0.0657 - val_mae: 0.1680\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0220 - mae: 0.1019 - val_loss: 0.0645 - val_mae: 0.1627\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0219 - mae: 0.0994 - val_loss: 0.0591 - val_mae: 0.1589\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0191 - mae: 0.0932 - val_loss: 0.0632 - val_mae: 0.1586\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0181 - mae: 0.0898 - val_loss: 0.0627 - val_mae: 0.1626\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0165 - mae: 0.0841 - val_loss: 0.0702 - val_mae: 0.1675\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0177 - mae: 0.0867 - val_loss: 0.0599 - val_mae: 0.1501\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0822 - val_loss: 0.0684 - val_mae: 0.1648\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.0827 - val_loss: 0.0582 - val_mae: 0.1503\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0141 - mae: 0.0783 - val_loss: 0.0643 - val_mae: 0.1635\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0155 - mae: 0.0830 - val_loss: 0.0590 - val_mae: 0.1514\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0133 - mae: 0.0768 - val_loss: 0.0569 - val_mae: 0.1497\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0118 - mae: 0.0720 - val_loss: 0.0603 - val_mae: 0.1552\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0118 - mae: 0.0726 - val_loss: 0.0592 - val_mae: 0.1515\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0107 - mae: 0.0690 - val_loss: 0.0596 - val_mae: 0.1544\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0118 - mae: 0.0718 - val_loss: 0.0570 - val_mae: 0.1468\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0110 - mae: 0.0690 - val_loss: 0.0638 - val_mae: 0.1590\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0119 - mae: 0.0725 - val_loss: 0.0604 - val_mae: 0.1517\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0104 - mae: 0.0685 - val_loss: 0.0580 - val_mae: 0.1506\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0093 - mae: 0.0652 - val_loss: 0.0612 - val_mae: 0.1478\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0085 - mae: 0.0621 - val_loss: 0.0579 - val_mae: 0.1478\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0083 - mae: 0.0599 - val_loss: 0.0594 - val_mae: 0.1512\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0083 - mae: 0.0608 - val_loss: 0.0627 - val_mae: 0.1515\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0080 - mae: 0.0598 - val_loss: 0.0618 - val_mae: 0.1505\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0078 - mae: 0.0613 - val_loss: 0.0580 - val_mae: 0.1482\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0080 - mae: 0.0608 - val_loss: 0.0619 - val_mae: 0.1539\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0092 - mae: 0.0655 - val_loss: 0.0614 - val_mae: 0.1528\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0091 - mae: 0.0641 - val_loss: 0.0623 - val_mae: 0.1553\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0080 - mae: 0.0631 - val_loss: 0.0580 - val_mae: 0.1487\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0080 - mae: 0.0618 - val_loss: 0.0625 - val_mae: 0.1556\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0640 - val_mae: 0.1581\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0063 - mae: 0.0558 - val_loss: 0.0582 - val_mae: 0.1442\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0065 - mae: 0.0556 - val_loss: 0.0635 - val_mae: 0.1564\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0073 - mae: 0.0581 - val_loss: 0.0579 - val_mae: 0.1465\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0068 - mae: 0.0577 - val_loss: 0.0589 - val_mae: 0.1471\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0056 - mae: 0.0513 - val_loss: 0.0586 - val_mae: 0.1451\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0056 - mae: 0.0506 - val_loss: 0.0574 - val_mae: 0.1439\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0057 - mae: 0.0521 - val_loss: 0.0578 - val_mae: 0.1426\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0056 - mae: 0.0513 - val_loss: 0.0587 - val_mae: 0.1462\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0052 - mae: 0.0496 - val_loss: 0.0596 - val_mae: 0.1445\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0045 - mae: 0.0462 - val_loss: 0.0589 - val_mae: 0.1458\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0480 - val_loss: 0.0582 - val_mae: 0.1455\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0050 - mae: 0.0492 - val_loss: 0.0619 - val_mae: 0.1499\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0052 - mae: 0.0502 - val_loss: 0.0565 - val_mae: 0.1447\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0053 - mae: 0.0499 - val_loss: 0.0599 - val_mae: 0.1516\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0062 - mae: 0.0528 - val_loss: 0.0571 - val_mae: 0.1461\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0051 - mae: 0.0497 - val_loss: 0.0577 - val_mae: 0.1454\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0051 - mae: 0.0506 - val_loss: 0.0569 - val_mae: 0.1442\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0052 - mae: 0.0498 - val_loss: 0.0592 - val_mae: 0.1458\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0043 - mae: 0.0450 - val_loss: 0.0585 - val_mae: 0.1449\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0047 - mae: 0.0471 - val_loss: 0.0584 - val_mae: 0.1441\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0048 - mae: 0.0477 - val_loss: 0.0595 - val_mae: 0.1499\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0050 - mae: 0.0492 - val_loss: 0.0562 - val_mae: 0.1426\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0486 - val_loss: 0.0603 - val_mae: 0.1497\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0048 - mae: 0.0477 - val_loss: 0.0585 - val_mae: 0.1450\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0484 - val_loss: 0.0562 - val_mae: 0.1420\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0043 - mae: 0.0455 - val_loss: 0.0568 - val_mae: 0.1430\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0048 - mae: 0.0466 - val_loss: 0.0638 - val_mae: 0.1537\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0053 - mae: 0.0498 - val_loss: 0.0602 - val_mae: 0.1462\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0043 - mae: 0.0461 - val_loss: 0.0568 - val_mae: 0.1387\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0489 - val_loss: 0.0564 - val_mae: 0.1425\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0051 - mae: 0.0507 - val_loss: 0.0583 - val_mae: 0.1421\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0043 - mae: 0.0466 - val_loss: 0.0557 - val_mae: 0.1373\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0040 - mae: 0.0451 - val_loss: 0.0570 - val_mae: 0.1403\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0042 - mae: 0.0453 - val_loss: 0.0529 - val_mae: 0.1385\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0049 - mae: 0.0481 - val_loss: 0.0558 - val_mae: 0.1414\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0044 - mae: 0.0462 - val_loss: 0.0563 - val_mae: 0.1405\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0048 - mae: 0.0485 - val_loss: 0.0564 - val_mae: 0.1417\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0049 - mae: 0.0500 - val_loss: 0.0566 - val_mae: 0.1421\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0044 - mae: 0.0462 - val_loss: 0.0605 - val_mae: 0.1465\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0046 - mae: 0.0477 - val_loss: 0.0583 - val_mae: 0.1454\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0052 - mae: 0.0486 - val_loss: 0.0591 - val_mae: 0.1456\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0052 - mae: 0.0501 - val_loss: 0.0565 - val_mae: 0.1475\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0056 - mae: 0.0524 - val_loss: 0.0561 - val_mae: 0.1388\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0047 - mae: 0.0485 - val_loss: 0.0534 - val_mae: 0.1378\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0046 - mae: 0.0479 - val_loss: 0.0586 - val_mae: 0.1466\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0041 - mae: 0.0452 - val_loss: 0.0574 - val_mae: 0.1364\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0430 - val_loss: 0.0545 - val_mae: 0.1383\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0039 - mae: 0.0432 - val_loss: 0.0553 - val_mae: 0.1394\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0040 - mae: 0.0457 - val_loss: 0.0555 - val_mae: 0.1397\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0039 - mae: 0.0447 - val_loss: 0.0557 - val_mae: 0.1405\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0041 - mae: 0.0455 - val_loss: 0.0540 - val_mae: 0.1356\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0036 - mae: 0.0421 - val_loss: 0.0560 - val_mae: 0.1406\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0423 - val_loss: 0.0557 - val_mae: 0.1426\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0043 - mae: 0.0460 - val_loss: 0.0576 - val_mae: 0.1404\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0039 - mae: 0.0428 - val_loss: 0.0551 - val_mae: 0.1399\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0034 - mae: 0.0416 - val_loss: 0.0563 - val_mae: 0.1388\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0035 - mae: 0.0410 - val_loss: 0.0539 - val_mae: 0.1374\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0039 - mae: 0.0441 - val_loss: 0.0582 - val_mae: 0.1413\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0037 - mae: 0.0443 - val_loss: 0.0549 - val_mae: 0.1384\n",
      "\n",
      "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
      "    is 0.0006674765467109243.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "best_hps, best_model, tuner, history = hyper_model(X_train,y_train_sc, X_val, y_val_sc,\n",
    "                                                        epoch=1000,factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "17/17 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_val = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0448\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0503 - mae: 0.1378\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0549 - mae: 0.1384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.054904562326678126, 0.13837788856672573]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_train, y_train_sc)\n",
    "best_model.evaluate(X_test, y_test_sc)\n",
    "best_model.evaluate(X_val, y_val_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # converting the predictions to certainity of 0 and 1 (May not be a good idea)\n",
    "# y_pred[:,-2][np.abs(y_pred[:,-2]) < 0.5] = 0\n",
    "# y_pred[:,-2][np.abs(y_pred[:,-2]) > 0.5] = 1\n",
    "# y_pred[:,-1][np.abs(y_pred[:,-1]) < 0.5] = 0\n",
    "# y_pred[:,-1][np.abs(y_pred[:,-1]) > 0.5] = 1\n",
    "\n",
    "# y_pred = pd.DataFrame(y_pred, columns=y_train.columns)\n",
    "# y1_pred_inverse = scaler_coords1.inverse_transform(y_pred[['x1', 'y1','x2', 'y2']])\n",
    "# y_pred[['x1', 'y1','x2', 'y2']] = pd.DataFrame(y1_pred_inverse,columns=['x1', 'y1','x2', 'y2'])\n",
    "# mse = score_mse(y_test, y_pred)\n",
    "# y_pred.loc[y_pred['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "# y_pred.loc[y_pred['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "\n",
    "# y_test.loc[y_test['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "# y_test.loc[y_test['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "# pd.concat([y_pred, y_test.reset_index().drop(columns='sample_number')], axis=1).to_csv(model_path+'predictions_nn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
