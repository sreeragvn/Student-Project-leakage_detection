{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN 6 outputs and status of swapping has to be decided on the basis of experiment 2\n",
    "model_path = 'saved_model/Multi_leak/experiment3/withswap/differentloss/'\n",
    "project_name='Multi_leak_experiment3_MTL_withSwap_differentloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 - 1 output layer with 1 loss function - mse. and do hyper parameter tuning.\n",
    "from utils.data_preprocess import load_data, load_single_leakage_model_data\n",
    "from utils.module import model_eval, hyper_model, model_comparison, linear_regression, numpy_to_tensor, benchmark_linear_model\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from kerastuner import HyperModel, Hyperband\n",
    "from keras import backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_leakage, two_leakage = load_data()\n",
    "# two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "# single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "# use the idea of masking instead\n",
    "data['x2'] = data['x2'].replace(np.nan, 8024)\n",
    "data['y2'] = data['y2'].replace(np.nan, 2616.5)\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual','total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', 'leak_2']]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', 'leak_2'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "# sort x1, y1 and x2, y2. coordinates with lowest x will take the position of x1\n",
    "def coords_swap(y1):\n",
    "    s = y1['x2'] < y1['x1']\n",
    "    y1.loc[s, ['x1','x2']] = y1.loc[s, ['x2','x1']].values\n",
    "    y1.loc[s, ['y1','y2']] = y1.loc[s, ['y2','y1']].values\n",
    "    return y1\n",
    "\n",
    "y1_train = y_train[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_train = y_train[[ 'leak_2']]\n",
    "y1_test = y_test[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_test = y_test[[ 'leak_2']]\n",
    "y1_val = y_val[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_val = y_val[[ 'leak_2']]\n",
    "\n",
    "# y1_test.to_csv('check1.csv')\n",
    "\n",
    "y1_data = [y1_train, y1_val, y1_test]\n",
    "y1_data_types = ['y1_train', 'y1_val', 'y1_test']\n",
    "for y1_data_types, y1 in zip(y1_data_types, y1_data):\n",
    "    y1_data_types = coords_swap(y1)\n",
    "\n",
    "# print()\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "# y1_test.to_csv('check2.csv')\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "y2_train = y2_train.reset_index().drop(columns='sample_number')\n",
    "y2_val = y2_val.reset_index().drop(columns='sample_number')\n",
    "y2_test = y2_test.reset_index().drop(columns='sample_number')\n",
    "\n",
    "y_train_all = pd.concat([y1_train, y2_train], axis=1)\n",
    "y_test_all = pd.concat([y1_test, y2_test], axis=1)\n",
    "y_val_all = pd.concat([y1_val, y2_val], axis=1)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, y1_train_np, y2_train_np = X_train.values, y1_train.values, y2_train.values\n",
    "X_val_np, y1_val_np, y2_val_np = X_val.values, y1_val.values, y2_val.values\n",
    "X_test_np, y1_test_np, y2_test_np = X_test.values, y1_test.values, y2_test.values\n",
    "\n",
    "# Create TensorFlow datasets from NumPy arrays.\n",
    "batch_size = 32\n",
    "buffer_size = len(X_train)  # Set the buffer size to the number of training examples for full shuffling.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y1_train_np, y2_train_np ))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y1_val_np, y2_val_np))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_np, y1_test_np, y2_test_np))\n",
    "\n",
    "# Shuffle, batch, and prefetch the training dataset.\n",
    "train_dataset_org = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch the validation and test datasets.\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = {\n",
    "#     \"y1\" : y1_train,\n",
    "#     \"y2\" : y2_train\n",
    "# }\n",
    "\n",
    "# y_val = {\n",
    "#     \"y1\" : y1_val,\n",
    "#     \"y2\" : y2_val\n",
    "# }\n",
    "\n",
    "# y_test = {\n",
    "#     \"y1\" : y1_test,\n",
    "#     \"y2\" : y2_test\n",
    "# }\n",
    "\n",
    "losses = {\n",
    "\t\"y1\": \"mse\",\n",
    "\t\"y2\": 'binary_crossentropy'\n",
    "    # \"y2\" : 'mse'\n",
    "    }\n",
    "\n",
    "metrics = {\n",
    "    \"y1\": 'mae',\n",
    "    \"y2\": 'mae'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 with Nadam\n",
    "\n",
    "EPOCHS = 1000\n",
    "# Define the multi-task HyperModel\n",
    "class MultiTaskHyperModel(HyperModel):\n",
    "\n",
    "    def build(self, hp):\n",
    "        backend.clear_session()\n",
    "        inputs = keras.Input(shape=(10,))\n",
    "        shared_layer = inputs\n",
    "        for i in range(hp.Int('num_layers', 1, 15)):\n",
    "            shared_layer = layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "                # activation=hp.Choice(\"activation\", [\"relu\", 'elu']),\n",
    "                activation = 'relu',\n",
    "                # add elu\n",
    "                kernel_initializer='he_uniform'\n",
    "            )(shared_layer)\n",
    "\n",
    "\n",
    "        task_layer1 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "            task_layer1 = layers.Dense(units=hp.Int(f'task_{i}_layer_{j}_neurons', min_value=8, max_value=128, step=8), \n",
    "                                       activation='relu',\n",
    "                                       kernel_initializer='he_uniform')(task_layer1)\n",
    "        y1 = layers.Dense(4, name='y1', activation = 'linear', kernel_initializer='he_uniform')(task_layer1)\n",
    "\n",
    "        task_layer2 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "            task_layer2 = layers.Dense(units=hp.Int(f'task_{i}_layer_{j}_neurons', min_value=2, max_value=64, step=2),\n",
    "                                        activation='relu',\n",
    "                                        kernel_initializer='he_uniform')(task_layer2)\n",
    "        y2 = layers.Dense(1, name='y2', activation = 'sigmoid', kernel_initializer='he_uniform')(task_layer2)\n",
    "\n",
    "        outputs = [y1, y2]\n",
    "\n",
    "        # loss1_weight = hp.Float(\"loss1_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "        # loss1_weight = hp.Float(\"loss2_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "        \n",
    "        loss1_weight = hp.Choice('loss1_weight', values=[0.6, 0.7, 0.8, 0.9])\n",
    "        # loss2_weight = hp.Choice('loss2_weight', values=[0.00005, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        # to stop the model from reducing both these weightage to very low and hence reducing the total loss\n",
    "        loss2_weight = 1 - loss1_weight\n",
    "\n",
    "        lossWeights = {\"y1\": loss1_weight, \"y2\": loss2_weight}\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "        # model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        #             loss=\"mse\",  metrics='mae')\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                    loss=losses, \n",
    "                    loss_weights=lossWeights,\n",
    "                    metrics = metrics)\n",
    "        return model\n",
    "\n",
    "# Create the multi-task HyperModel\n",
    "multi_task_hypermodel = MultiTaskHyperModel()\n",
    "\n",
    "# Define the Hyperband tuner\n",
    "tuner = Hyperband(\n",
    "    multi_task_hypermodel,\n",
    "    objective =  kt.Objective(\"val_y1_mae\", direction=\"min\"),\n",
    "    max_epochs=EPOCHS+100,\n",
    "    factor=2,\n",
    "    directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "    project_name=project_name,\n",
    "    seed=0,   \n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train, \n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), \n",
    "             verbose = 1, callbacks=[stop_early],\n",
    "             epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hps)\n",
    "\n",
    "# Build the best model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "history = best_model.fit(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train,\n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),  \n",
    "            # callbacks=[stop_early],\n",
    "            verbose = 1, epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
    "    is {best_model.optimizer.lr.numpy()}.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 320)          3520        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 288)          92448       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 352)          101728      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 384)          135552      ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 192)          73920       ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 512)          98816       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 80)           41040       ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 80)           41040       ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 16)           1296        ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 16)           1296        ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " y1 (Dense)                     (None, 4)            68          ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " y2 (Dense)                     (None, 1)            17          ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 590,741\n",
      "Trainable params: 590,741\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# best_model.save(model_path)\n",
    "best_model = tf.keras.models.load_model(model_path)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "    0.1521     0.1398     0.1041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_predictions_train = best_model.predict(train_dataset_org.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})))\n",
    "# print(\"train\", \"{:10.4f}\".format(mean_squared_error(y_train, y_predictions, squared=True)))\n",
    "y_predictions_val = best_model.predict(val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})))\n",
    "# print(\"val\", \"{:10.4f}\".format(mean_squared_error(y_val, y_predictions, squared=True)))\n",
    "y_predictions = best_model.predict(test_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})))\n",
    "\n",
    "y_predictions_train = np.concatenate((y_predictions_train[0], y_predictions_train[1]), axis=1)\n",
    "y_predictions_val = np.concatenate((y_predictions_val[0], y_predictions_val[1]), axis=1)\n",
    "y_predictions = np.concatenate((y_predictions[0], y_predictions[1]), axis=1)\n",
    "\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) < 0.5] = 0\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) < 0.5] = 0\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) < 0.5] = 0\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) > 0.5] = 1\n",
    "\n",
    "loss_test = \"{:10.4f}\".format(mean_squared_error(y_test_all, y_predictions, squared=True))\n",
    "metric_test = \"{:10.4f}\".format(mean_absolute_error(y_test_all, y_predictions))\n",
    "\n",
    "loss_val = \"{:10.4f}\".format(mean_squared_error(y_val_all, y_predictions_val, squared=True))\n",
    "metric_val = \"{:10.4f}\".format(mean_absolute_error(y_val_all, y_predictions_val))\n",
    "\n",
    "loss_train = \"{:10.4f}\".format(mean_squared_error(y_train_all, y_predictions_train, squared=True))\n",
    "metric_train = \"{:10.4f}\".format(mean_absolute_error(y_train_all, y_predictions_train))\n",
    "\n",
    "print(metric_test, metric_val, metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = best_model.evaluate(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), verbose=1)\n",
    "results_val = best_model.evaluate(val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), verbose=1)\n",
    "results_test = best_model.evaluate(test_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
