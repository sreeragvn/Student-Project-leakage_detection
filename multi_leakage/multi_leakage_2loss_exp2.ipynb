{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocess import load_data, load_single_leakage_model_data\n",
    "from utils.module import model_eval, hyper_model, model_comparison, linear_regression, numpy_to_tensor, benchmark_linear_model\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from kerastuner import HyperModel, Hyperband\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_model/Multi_leak/2_loss/Mid_structure_anchor/6 Output/exp2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_multi.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.full_load(ymlfile)\n",
    "\n",
    "\n",
    "single_leakage, two_leakage = load_data(total_samples = cfg['experiment']['total_samples'])\n",
    "two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "data['x2'] = data['x2'].replace(np.nan, 8024)\n",
    "data['y2'] = data['y2'].replace(np.nan, 2616.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual', 'tot_residual_flow', \n",
    "       'total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2']]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "y1_train = y_train[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_train = y_train[['leak_1', 'leak_2']]\n",
    "y1_test = y_test[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_test = y_test[['leak_1', 'leak_2']]\n",
    "y1_val = y_val[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_val = y_val[['leak_1', 'leak_2']]\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "# y1_train['x2'] = y1_train['x2'].replace(np.nan, -5)\n",
    "# y1_train['y2'] = y1_train['y2'].replace(np.nan, -5)\n",
    "\n",
    "# y1_test['x2'] = y1_test['x2'].replace(np.nan, -5)\n",
    "# y1_test['y2'] = y1_test['y2'].replace(np.nan, -5)\n",
    "\n",
    "# y1_val['x2'] = y1_val['x2'].replace(np.nan, -5)\n",
    "# y1_val['y2'] = y1_val['y2'].replace(np.nan, -5)\n",
    "# Not sure if 0 is good enough or try generating a random number\n",
    "\n",
    "# scaler_coords2 = StandardScaler()\n",
    "# y2_train = scaler_coords2.fit_transform(y2_train)\n",
    "# y2_test = scaler_coords2.fit_transform(y2_test)\n",
    "# y2_val = scaler_coords2.transform(y2_val)\n",
    "\n",
    "# y2_train = pd.DataFrame(y2_train, columns=y2_columns)\n",
    "# y2_test = pd.DataFrame(y2_test, columns=y2_columns)\n",
    "# y2_val = pd.DataFrame(y2_val, columns=y2_columns)\n",
    "\n",
    "y2_train = y2_train.reset_index().drop(columns='sample_number')\n",
    "y2_val = y2_val.reset_index().drop(columns='sample_number')\n",
    "y2_test = y2_test.reset_index().drop(columns='sample_number')\n",
    "\n",
    "# y_train = pd.concat([y1_train, y2_train], axis=1)\n",
    "# y_test = pd.concat([y1_test, y2_test], axis=1)\n",
    "# y_val = pd.concat([y1_val, y2_val], axis=1)\n",
    "\n",
    "# scaler_flows = StandardScaler()\n",
    "# X_train = scaler_flows.fit_transform(X_train)\n",
    "# X_test = scaler_flows.transform(X_test)\n",
    "# X_val = scaler_flows.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, y1_train_np, y2_train_np = X_train.values, y1_train.values, y2_train.values\n",
    "X_val_np, y1_val_np, y2_val_np = X_val.values, y1_val.values, y2_val.values\n",
    "X_test_np, y1_test_np, y2_test_np = X_test.values, y1_test.values, y2_test.values\n",
    "\n",
    "# Create TensorFlow datasets from NumPy arrays.\n",
    "batch_size = 32\n",
    "buffer_size = len(X_train)  # Set the buffer size to the number of training examples for full shuffling.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y1_train_np, y2_train_np ))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y1_val_np, y2_val_np))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_np, y1_test_np, y2_test_np))\n",
    "\n",
    "# Shuffle, batch, and prefetch the training dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch the validation and test datasets.\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = {\n",
    "#     \"y1\" : y1_train,\n",
    "#     \"y2\" : y2_train\n",
    "# }\n",
    "\n",
    "# y_val = {\n",
    "#     \"y1\" : y1_val,\n",
    "#     \"y2\" : y2_val\n",
    "# }\n",
    "\n",
    "# y_test = {\n",
    "#     \"y1\" : y1_test,\n",
    "#     \"y2\" : y2_test\n",
    "# }\n",
    "\n",
    "losses = {\n",
    "\t\"y1\": \"mse\",\n",
    "\t\"y2\": 'binary_crossentropy'\n",
    "    # \"y2\" : 'mse'\n",
    "    }\n",
    "\n",
    "metrics = {\n",
    "    \"y1\": 'mae',\n",
    "    \"y2\": 'mae'\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def base_model(inputs):\n",
    "#     # add the sequential layers here\n",
    "#     x= Dense(128, activation='relu', kernel_initializer='he_uniform')(inputs)\n",
    "#     # x= Dense(128, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "#     return x\n",
    "\n",
    "# def final_model(inputs):\n",
    "#     x = base_model(inputs)\n",
    "#     y1 = Dense(units=4, name='y1')(x)\n",
    "#     y2 = Dense(units =2, name = 'y2')(x)\n",
    "#     model = Model(inputs=inputs, outputs = [y1, y2])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# inputs = tf.keras.layers.Input(shape=(10,))\n",
    "# model = final_model(inputs)\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss=losses,\n",
    "#             metrics = metrics)\n",
    "\n",
    "# history = model.fit(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), \n",
    "#                     validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), \n",
    "#                     verbose = 1, epochs=100, shuffle = True)\n",
    "# # model_evaluate, y_pred = model_eval(model, X_test, y_test, X_train, y_train, X_val, y_val)\n",
    "# # coords = np.concatenate([y_pred[0], y_test['y1']], axis=1)\n",
    "# # presence = np.concatenate([y_pred[1], y_test['y2']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 with Nadam\n",
    "\n",
    "EPOCHS = 1000\n",
    "# Define the multi-task HyperModel\n",
    "class MultiTaskHyperModel(HyperModel):\n",
    "\n",
    "    def build(self, hp):\n",
    "        inputs = keras.Input(shape=(10,))\n",
    "        shared_layer = inputs\n",
    "        for i in range(hp.Int('num_layers', 1, 15)):\n",
    "            shared_layer = layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "                # activation=hp.Choice(\"activation\", [\"relu\"]),\n",
    "                activation = 'relu',\n",
    "                # add elu\n",
    "                kernel_initializer='he_uniform'\n",
    "            )(shared_layer)\n",
    "\n",
    "\n",
    "        task_layer1 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "            task_layer1 = layers.Dense(units=hp.Choice(f'task_{i}_layer_{j}_neurons', values=[4, 8, 16]), \n",
    "                                       activation='relu',\n",
    "                                       kernel_initializer='he_uniform')(task_layer1)\n",
    "        y1 = layers.Dense(4, name='y1', activation = 'linear', kernel_initializer='he_uniform')(task_layer1)\n",
    "\n",
    "        task_layer2 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "            task_layer2 = layers.Dense(units=hp.Choice(f'task_{i}_layer_{j}_neurons', values=[4, 8, 16]),\n",
    "                                        activation='relu',\n",
    "                                        kernel_initializer='he_uniform')(task_layer2)\n",
    "        y2 = layers.Dense(2, name='y2', activation = 'sigmoid', kernel_initializer='he_uniform')(task_layer2)\n",
    "\n",
    "        outputs = [y1, y2]\n",
    "\n",
    "        # loss1_weight = hp.Float(\"loss1_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "        # loss1_weight = hp.Float(\"loss2_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "        \n",
    "        loss1_weight = hp.Choice('loss1_weight', values=[0.2, 0.4, 0.6, 0.8])\n",
    "        # loss2_weight = hp.Choice('loss2_weight', values=[0.00005, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        # to stop the model from reducing both these weightage to very low and hence reducing the total loss\n",
    "        loss2_weight = 1 - loss1_weight\n",
    "\n",
    "        lossWeights = {\"y1\": loss1_weight, \"y2\": loss2_weight}\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "        # model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        #             loss=\"mse\",  metrics='mae')\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                    loss=losses, loss_weights=lossWeights,\n",
    "                    metrics = metrics)\n",
    "        return model\n",
    "\n",
    "# Create the multi-task HyperModel\n",
    "multi_task_hypermodel = MultiTaskHyperModel()\n",
    "\n",
    "# Define the Hyperband tuner\n",
    "tuner = Hyperband(\n",
    "    multi_task_hypermodel,\n",
    "    objective = kt.Objective(\"val_y1_mae\", direction=\"min\"),\n",
    "    max_epochs=EPOCHS,\n",
    "    factor=2,\n",
    "    directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "    project_name='multi_task_tuning_NAdam_obj_val_y1_mae'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train, \n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), \n",
    "             verbose = 1, epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hps)\n",
    "\n",
    "# Build the best model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "best_model.fit(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})),\n",
    "            #  X_train, y_train, \n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), \n",
    "             verbose = 1, epochs=EPOCHS, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 128)          1408        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 32)           4128        ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 192)          6336        ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 288)          55584       ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 192)          55488       ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 480)          92640       ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 448)          215488      ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 512)          229888      ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 384)          196992      ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 192)          73920       ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 160)          30880       ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 384)          61824       ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 224)          86240       ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 64)           14400       ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 16)           1040        ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 16)           1040        ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 16)           272         ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 16)           272         ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 16)           272         ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 16)           272         ['dense_39[0][0]']               \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 16)           272         ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 16)           272         ['dense_40[0][0]']               \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 8)            136         ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " dense_42 (Dense)               (None, 8)            136         ['dense_41[0][0]']               \n",
      "                                                                                                  \n",
      " y1 (Dense)                     (None, 4)            36          ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " y2 (Dense)                     (None, 2)            18          ['dense_42[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,129,254\n",
      "Trainable params: 1,129,254\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0078 - y1_loss: 0.0194 - y2_loss: 1.8414e-06 - y1_mae: 0.0729 - y2_mae: 1.8412e-06\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2148 - y1_loss: 0.5371 - y2_loss: 7.1053e-07 - y1_mae: 0.3491 - y2_mae: 7.1052e-07\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1857 - y1_loss: 0.4643 - y2_loss: 4.4306e-06 - y1_mae: 0.3524 - y2_mae: 4.4300e-06\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(model_path)\n",
    "best_model.summary()\n",
    "y_predictions = best_model.predict(test_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})))\n",
    "results_train = best_model.evaluate(train_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), verbose=1)\n",
    "results_val = best_model.evaluate(val_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), verbose=1)\n",
    "results_test = best_model.evaluate(test_dataset.map(lambda x, y1, y2: (x, {'y1': y1, 'y2': y2})), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((y_predictions[0],y_predictions[1]), axis=1)\n",
    "y_pred[:,-2][np.abs(y_pred[:,-2]) < 0.5] = 0\n",
    "y_pred[:,-2][np.abs(y_pred[:,-2]) > 0.5] = 1\n",
    "y_pred[:,-1][np.abs(y_pred[:,-1]) < 0.5] = 0\n",
    "y_pred[:,-1][np.abs(y_pred[:,-1]) > 0.5] = 1\n",
    "y_pred = pd.DataFrame(y_pred, columns=y_train.columns)\n",
    "y1_pred_inverse = scaler_coords1.inverse_transform(y_pred[['x1', 'y1','x2', 'y2']])\n",
    "y_pred[['x1', 'y1','x2', 'y2']] = pd.DataFrame(y1_pred_inverse,columns=['x1', 'y1','x2', 'y2'])\n",
    "pd.concat([y_pred, y_test.reset_index().drop(columns='sample_number')], axis=1).to_csv(model_path+'predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      2\u001b[0m y_test_scale \u001b[39m=\u001b[39m scaler_coords1\u001b[39m.\u001b[39mtransform(y_test[[\u001b[39m'\u001b[39m\u001b[39mx1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mx2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my2\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[0;32m----> 3\u001b[0m y_test_scale[[\u001b[39m'\u001b[39m\u001b[39mx1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mx2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my2\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(y_test_scale,columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mx1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mx2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my2\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m mse_x1 \u001b[39m=\u001b[39m mean_squared_error(y_test_scale[\u001b[39m'\u001b[39m\u001b[39mx1\u001b[39m\u001b[39m'\u001b[39m], y_pred[\u001b[39m'\u001b[39m\u001b[39mx1\u001b[39m\u001b[39m'\u001b[39m], squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m mse_x2 \u001b[39m=\u001b[39m mean_squared_error(y_test_scale[\u001b[39m'\u001b[39m\u001b[39mx2\u001b[39m\u001b[39m'\u001b[39m], y_pred[\u001b[39m'\u001b[39m\u001b[39mx2\u001b[39m\u001b[39m'\u001b[39m], squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_x1 = mean_squared_error(y_test['x1'], y_pred['x1'], squared=True)\n",
    "mse_x2 = mean_squared_error(y_test['x2'], y_pred['x2'], squared=True)\n",
    "\n",
    "mse_y1 = mean_squared_error(y_test['y1'], y_pred['y1'], squared=True)\n",
    "mse_y2 = mean_squared_error(y_test['y2'], y_pred['y2'], squared=True)\n",
    "\n",
    "mse_leak1 = mean_squared_error(y_test['leak_1'], y_pred['leak_1'], squared=True)\n",
    "mse_leak2 = mean_squared_error(y_test['leak_2'], y_pred['leak_1'], squared=True)\n",
    "\n",
    "mse = [mse_x1, mse_x2, mse_y1, mse_y2, mse_leak1, mse_leak2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83856294.69196765,\n",
       " 71068582.13663048,\n",
       " 9592381.8191516,\n",
       " 7817903.647545287,\n",
       " 0.0,\n",
       " 0.5147058823529411]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# idea to explore - \n",
    "# 1. Just use 1 loss function for the entire output\n",
    "# 2. Give 2 loss function for each leakageness and coordinates (2a. Try the same without sigmoid function)\n",
    "# 3.\n",
    "# create a model first with 4 coordinate - 2 for each. \n",
    "# Then use that model as base model, add additional layers for leakageness and do transfer learning to do the 6 output case\n",
    "\n",
    "# 4. \n",
    "# use masking feature of tensorflow (low priority) - # Use the idea of masking for the case where when we have to only 1 leakage \n",
    "\n",
    "# do i actually need objectness 1. since it is always present.i just need 1 objectness right ?\n",
    "# coordinate are more important right - hence give more wightage to the coordinate loss function\n",
    "# try auxiliary task and joint model\n",
    "# remove bad quality data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
