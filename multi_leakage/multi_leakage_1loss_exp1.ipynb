{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.1\n",
      "# GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 04:52:38.319923: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2 - 1 output layer with 1 loss function - mse. and do hyper parameter tuning.\n",
    "from utils.data_preprocess import load_data, load_single_leakage_model_data\n",
    "from utils.module import model_eval, hyper_model, model_comparison, linear_regression, numpy_to_tensor, benchmark_linear_model\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_model/Multi_leak/1_loss_linear_reg/'\n",
    "project_name='multileak_6out_1loss_withswap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_multi.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.full_load(ymlfile)\n",
    "\n",
    "\n",
    "single_leakage, two_leakage = load_data(total_samples = cfg['experiment']['total_samples'])\n",
    "two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "data['x2'] = data['x2'].replace(np.nan, 8024)\n",
    "data['y2'] = data['y2'].replace(np.nan, 2616.5)\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual', 'tot_residual_flow', \n",
    "       'total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2']]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "y1_train = y_train[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_train = y_train[['leak_1', 'leak_2']]\n",
    "y1_test = y_test[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_test = y_test[['leak_1', 'leak_2']]\n",
    "y1_val = y_val[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_val = y_val[['leak_1', 'leak_2']]\n",
    "\n",
    "def coords_swap(y1):\n",
    "    s = y1['x2'] < y1['x1']\n",
    "    y1.loc[s, ['x1','x2']] = y1.loc[s, ['x2','x1']].values\n",
    "    y1.loc[s, ['y1','y2']] = y1.loc[s, ['y2','y1']].values\n",
    "    return y1\n",
    "\n",
    "y1_data = [y1_train, y1_val, y1_test]\n",
    "y1_data_types = ['y1_train', 'y1_val', 'y1_test']\n",
    "for y1_data_types, y1 in zip(y1_data_types, y1_data):\n",
    "    y1_data_types = coords_swap(y1)\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "# y1_train['x2'] = y1_train['x2'].replace(np.nan, -5)\n",
    "# y1_train['y2'] = y1_train['y2'].replace(np.nan, -5)\n",
    "\n",
    "# y1_test['x2'] = y1_test['x2'].replace(np.nan, -5)\n",
    "# y1_test['y2'] = y1_test['y2'].replace(np.nan, -5)\n",
    "\n",
    "# y1_val['x2'] = y1_val['x2'].replace(np.nan, -5)\n",
    "# y1_val['y2'] = y1_val['y2'].replace(np.nan, -5)\n",
    "# Not sure if 0 is good enough or try generating a random number\n",
    "\n",
    "# scaler_coords2 = StandardScaler()\n",
    "# y2_train = scaler_coords2.fit_transform(y2_train)\n",
    "# y2_test = scaler_coords2.fit_transform(y2_test)\n",
    "# y2_val = scaler_coords2.transform(y2_val)\n",
    "\n",
    "# y2_train = pd.DataFrame(y2_train, columns=y2_columns)\n",
    "# y2_test = pd.DataFrame(y2_test, columns=y2_columns)\n",
    "# y2_val = pd.DataFrame(y2_val, columns=y2_columns)\n",
    "\n",
    "y2_train = y2_train.reset_index().drop(columns='sample_number')\n",
    "y2_val = y2_val.reset_index().drop(columns='sample_number')\n",
    "y2_test = y2_test.reset_index().drop(columns='sample_number')\n",
    "\n",
    "y_train_sc = pd.concat([y1_train, y2_train], axis=1)\n",
    "y_test_sc = pd.concat([y1_test, y2_test], axis=1)\n",
    "y_val_sc = pd.concat([y1_val, y2_val], axis=1)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mse(y_test, y_pred):\n",
    "    mse_x1 = mean_squared_error(y_test['x1'], y_pred['x1'], squared=True)\n",
    "    mse_x2 = mean_squared_error(y_test['x2'], y_pred['x2'], squared=True)\n",
    "\n",
    "    mse_y1 = mean_squared_error(y_test['y1'], y_pred['y1'], squared=True)\n",
    "    mse_y2 = mean_squared_error(y_test['y2'], y_pred['y2'], squared=True)\n",
    "\n",
    "    mse_leak1 = mean_squared_error(y_test['leak_1'], y_pred['leak_1'], squared=True)\n",
    "    mse_leak2 = mean_squared_error(y_test['leak_2'], y_pred['leak_2'], squared=True)\n",
    "\n",
    "    mse = [mse_x1, mse_x2, mse_y1, mse_y2, mse_leak1, mse_leak2]\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple linear regression model\n",
    "reg = LinearRegression().fit(X_train, y_train_sc)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# converting the predictions to certainity of 0 and 1 (May not be a good idea)\n",
    "y_pred[:,-2][np.abs(y_pred[:,-2]) < 0.5] = 0\n",
    "y_pred[:,-2][np.abs(y_pred[:,-2]) > 0.5] = 1\n",
    "y_pred[:,-1][np.abs(y_pred[:,-1]) < 0.5] = 0\n",
    "y_pred[:,-1][np.abs(y_pred[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred, columns=y_train.columns)\n",
    "y1_pred_inverse = scaler_coords1.inverse_transform(y_pred[['x1', 'y1','x2', 'y2']])\n",
    "y_pred[['x1', 'y1','x2', 'y2']] = pd.DataFrame(y1_pred_inverse,columns=['x1', 'y1','x2', 'y2'])\n",
    "mse = score_mse(y_test, y_pred)\n",
    "y_pred.loc[y_pred['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "y_pred.loc[y_pred['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "\n",
    "y_test.loc[y_test['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "y_test.loc[y_test['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "pd.concat([y_pred, y_test.reset_index().drop(columns='sample_number')], axis=1).to_csv(model_path+'predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12625452.202204369,\n",
       " 12499811.327783069,\n",
       " 1031929.5817594002,\n",
       " 908347.861655197,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model using model builder from keras tuner\n",
    "def model_builder_single(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Choose an optimal value between 32-512\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 15)):\n",
    "        l1_weight = hp.Choice('l1_weight', values=[0.0, 1e-1, 1e-2, 1e-3])\n",
    "        l2_weight = hp.Choice('l2_weight', values=[0.0, 1e-1, 1e-2, 1e-3])\n",
    "        kernel_regularizer=keras.regularizers.L1L2(l1 = l1_weight, l2 = l2_weight)\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "                kernel_initializer='he_uniform',\n",
    "                kernel_regularizer=kernel_regularizer\n",
    "            )\n",
    "        )\n",
    "    model.add(keras.layers.Dense(units=6, activation= \"linear\", kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    # hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "    # model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    #             loss=\"mse\",  metrics='mae')\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                loss=\"mse\",\n",
    "                metrics='mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#search for the best hyperparameters and train the standard model with original training data\n",
    "def hyper_model(X_train,Y_train, X_val, y_val, epoch, factor, augmentation, residual_subtract, \n",
    "                blind_flip, tot_flow, res_flow, tot_resflow):\n",
    "    folder_name = project_name\n",
    "    tuner = kt.Hyperband(model_builder_single,\n",
    "                         objective='val_loss',\n",
    "                         max_epochs=epoch,\n",
    "                         factor=factor,\n",
    "                         hyperband_iterations = 1,\n",
    "                        # Integer, at least 1, the number of times to iterate over the full Hyperband algorithm. One iteration will \n",
    "                        # run approximately max_epochs * (math.log(max_epochs, factor) ** 2) cumulative epochs across all trials. It is \n",
    "                        # recommended to set this to as high a value as is within your resource budget. Defaults to 1.\n",
    "                         directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "                         seed=0,    \n",
    "                         project_name=str(folder_name))\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    tuner.search(X_train, Y_train, epochs=epoch, validation_data = (X_val, y_val), callbacks=[stop_early, \n",
    "                                                                                            #   keras.callbacks.TensorBoard(\"../tensorflow_log_files/studienarbeit/tb_logs\"+str(folder_name))\n",
    "                                                                                              ])\n",
    "    #tuner.search(X_train, Y_train, epochs=50, validation_data=(X_test,Y_test), callbacks=[stop_early])\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(X_train, Y_train, epochs=epoch, validation_data = (X_val, y_val), shuffle= True)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
    "    is {model.optimizer.lr.numpy()}.\n",
    "    \"\"\")\n",
    "\n",
    "    return best_hps, model, tuner, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 356 Complete [00h 00m 11s]\n",
      "val_loss: 1.4213231991932072\n",
      "\n",
      "Best val_loss So Far: 0.04231417227884822\n",
      "Total elapsed time: 00h 09m 11s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 2s 12ms/step - loss: 0.4696 - mae: 0.4946 - val_loss: 0.2216 - val_mae: 0.3655\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1211 - mae: 0.2474 - val_loss: 0.1362 - val_mae: 0.2653\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0821 - mae: 0.1991 - val_loss: 0.1543 - val_mae: 0.2735\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0728 - mae: 0.1859 - val_loss: 0.0831 - val_mae: 0.1965\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0528 - mae: 0.1553 - val_loss: 0.0762 - val_mae: 0.1816\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0422 - mae: 0.1383 - val_loss: 0.0737 - val_mae: 0.1766\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0372 - mae: 0.1293 - val_loss: 0.0698 - val_mae: 0.1711\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0332 - mae: 0.1230 - val_loss: 0.0736 - val_mae: 0.1695\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0332 - mae: 0.1218 - val_loss: 0.0922 - val_mae: 0.2010\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0344 - mae: 0.1241 - val_loss: 0.0724 - val_mae: 0.1762\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.1137 - val_loss: 0.0817 - val_mae: 0.1874\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.1205 - val_loss: 0.0824 - val_mae: 0.1840\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0284 - mae: 0.1130 - val_loss: 0.0695 - val_mae: 0.1746\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0255 - mae: 0.1092 - val_loss: 0.0647 - val_mae: 0.1625\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0205 - mae: 0.0953 - val_loss: 0.0514 - val_mae: 0.1411\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0888 - val_loss: 0.0599 - val_mae: 0.1531\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0168 - mae: 0.0871 - val_loss: 0.0510 - val_mae: 0.1418\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0151 - mae: 0.0836 - val_loss: 0.0587 - val_mae: 0.1547\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0154 - mae: 0.0856 - val_loss: 0.0533 - val_mae: 0.1466\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0138 - mae: 0.0804 - val_loss: 0.0551 - val_mae: 0.1463\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0129 - mae: 0.0784 - val_loss: 0.0504 - val_mae: 0.1434\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0138 - mae: 0.0796 - val_loss: 0.0545 - val_mae: 0.1442\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0120 - mae: 0.0757 - val_loss: 0.0543 - val_mae: 0.1431\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0130 - mae: 0.0794 - val_loss: 0.0569 - val_mae: 0.1541\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0148 - mae: 0.0857 - val_loss: 0.0502 - val_mae: 0.1388\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0118 - mae: 0.0764 - val_loss: 0.0675 - val_mae: 0.1636\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0149 - mae: 0.0830 - val_loss: 0.0562 - val_mae: 0.1478\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0110 - mae: 0.0735 - val_loss: 0.0555 - val_mae: 0.1449\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0095 - mae: 0.0691 - val_loss: 0.0504 - val_mae: 0.1335\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0096 - mae: 0.0680 - val_loss: 0.0588 - val_mae: 0.1510\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0709 - val_loss: 0.0571 - val_mae: 0.1458\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0118 - mae: 0.0745 - val_loss: 0.0559 - val_mae: 0.1438\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0092 - mae: 0.0676 - val_loss: 0.0553 - val_mae: 0.1411\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0711 - val_loss: 0.0484 - val_mae: 0.1334\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0088 - mae: 0.0660 - val_loss: 0.0579 - val_mae: 0.1473\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0097 - mae: 0.0677 - val_loss: 0.0534 - val_mae: 0.1378\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0084 - mae: 0.0649 - val_loss: 0.0526 - val_mae: 0.1381\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0700 - val_loss: 0.0535 - val_mae: 0.1384\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0074 - mae: 0.0614 - val_loss: 0.0570 - val_mae: 0.1436\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0089 - mae: 0.0649 - val_loss: 0.0532 - val_mae: 0.1386\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0081 - mae: 0.0631 - val_loss: 0.0542 - val_mae: 0.1436\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0667 - val_loss: 0.0543 - val_mae: 0.1435\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0074 - mae: 0.0616 - val_loss: 0.0534 - val_mae: 0.1410\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0069 - mae: 0.0590 - val_loss: 0.0521 - val_mae: 0.1324\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0052 - mae: 0.0512 - val_loss: 0.0500 - val_mae: 0.1300\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0049 - mae: 0.0497 - val_loss: 0.0513 - val_mae: 0.1336\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0053 - mae: 0.0525 - val_loss: 0.0501 - val_mae: 0.1297\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0046 - mae: 0.0476 - val_loss: 0.0473 - val_mae: 0.1280\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0044 - mae: 0.0471 - val_loss: 0.0773 - val_mae: 0.1705\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0091 - mae: 0.0644 - val_loss: 0.0506 - val_mae: 0.1303\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0056 - mae: 0.0537 - val_loss: 0.0746 - val_mae: 0.1630\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0094 - mae: 0.0647 - val_loss: 0.0504 - val_mae: 0.1351\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0057 - mae: 0.0547 - val_loss: 0.0527 - val_mae: 0.1401\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0052 - mae: 0.0513 - val_loss: 0.0625 - val_mae: 0.1429\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0061 - mae: 0.0540 - val_loss: 0.0486 - val_mae: 0.1296\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0060 - mae: 0.0544 - val_loss: 0.0600 - val_mae: 0.1507\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0075 - mae: 0.0606 - val_loss: 0.0502 - val_mae: 0.1348\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0662 - val_loss: 0.0532 - val_mae: 0.1433\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0068 - mae: 0.0599 - val_loss: 0.0534 - val_mae: 0.1340\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0047 - mae: 0.0490 - val_loss: 0.0543 - val_mae: 0.1383\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0051 - mae: 0.0505 - val_loss: 0.0496 - val_mae: 0.1268\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0042 - mae: 0.0459 - val_loss: 0.0503 - val_mae: 0.1349\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0041 - mae: 0.0463 - val_loss: 0.0508 - val_mae: 0.1304\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0035 - mae: 0.0416 - val_loss: 0.0536 - val_mae: 0.1335\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0037 - mae: 0.0422 - val_loss: 0.0513 - val_mae: 0.1302\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0035 - mae: 0.0419 - val_loss: 0.0498 - val_mae: 0.1257\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0034 - mae: 0.0413 - val_loss: 0.0548 - val_mae: 0.1418\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0047 - mae: 0.0475 - val_loss: 0.0507 - val_mae: 0.1313\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0037 - mae: 0.0435 - val_loss: 0.0514 - val_mae: 0.1326\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0035 - mae: 0.0425 - val_loss: 0.0511 - val_mae: 0.1298\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0035 - mae: 0.0414 - val_loss: 0.0600 - val_mae: 0.1453\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0051 - mae: 0.0476 - val_loss: 0.0531 - val_mae: 0.1321\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0044 - mae: 0.0467 - val_loss: 0.0495 - val_mae: 0.1295\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0042 - mae: 0.0460 - val_loss: 0.0539 - val_mae: 0.1327\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0035 - mae: 0.0424 - val_loss: 0.0522 - val_mae: 0.1295\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0038 - mae: 0.0423 - val_loss: 0.0515 - val_mae: 0.1292\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0038 - mae: 0.0425 - val_loss: 0.0517 - val_mae: 0.1322\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0035 - mae: 0.0420 - val_loss: 0.0500 - val_mae: 0.1275\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0046 - mae: 0.0463 - val_loss: 0.0484 - val_mae: 0.1339\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0059 - mae: 0.0546 - val_loss: 0.0538 - val_mae: 0.1365\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0053 - mae: 0.0521 - val_loss: 0.0523 - val_mae: 0.1316\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0054 - mae: 0.0515 - val_loss: 0.0564 - val_mae: 0.1381\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0128 - mae: 0.0697 - val_loss: 0.0502 - val_mae: 0.1370\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0085 - mae: 0.0650 - val_loss: 0.0623 - val_mae: 0.1564\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0704 - val_loss: 0.0540 - val_mae: 0.1385\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0068 - mae: 0.0598 - val_loss: 0.0550 - val_mae: 0.1417\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0063 - mae: 0.0574 - val_loss: 0.0541 - val_mae: 0.1366\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0053 - mae: 0.0527 - val_loss: 0.0521 - val_mae: 0.1354\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0050 - mae: 0.0497 - val_loss: 0.0502 - val_mae: 0.1282\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0048 - mae: 0.0491 - val_loss: 0.0491 - val_mae: 0.1302\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0038 - mae: 0.0450 - val_loss: 0.0527 - val_mae: 0.1321\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0037 - mae: 0.0425 - val_loss: 0.0534 - val_mae: 0.1354\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0036 - mae: 0.0428 - val_loss: 0.0569 - val_mae: 0.1365\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0059 - mae: 0.0539 - val_loss: 0.0503 - val_mae: 0.1306\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0048 - mae: 0.0501 - val_loss: 0.0510 - val_mae: 0.1276\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0040 - mae: 0.0458 - val_loss: 0.0495 - val_mae: 0.1272\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0046 - mae: 0.0471 - val_loss: 0.0483 - val_mae: 0.1251\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0042 - mae: 0.0448 - val_loss: 0.0507 - val_mae: 0.1268\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0035 - mae: 0.0418 - val_loss: 0.0521 - val_mae: 0.1281\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0031 - mae: 0.0407 - val_loss: 0.0481 - val_mae: 0.1263\n",
      "\n",
      "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
      "    is 0.0006674765467109243.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "best_hps, best_model, tuner, history = hyper_model(X_train,y_train_sc, X_val, y_val_sc,\n",
    "                                                        cfg['experiment']['EPOCH'], cfg['experiment']['factor'], False, \n",
    "                                                        False, False, False, False, 'False_two_leak_1loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Multi_leak/1_loss_linear_reg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Multi_leak/1_loss_linear_reg/assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), -2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/pandas/_libs/index.pyx:144\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), -2)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# converting the predictions to certainity of 0 and 1 (May not be a good idea)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred[:,\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m][np\u001b[39m.\u001b[39mabs(y_pred[:,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m0.5\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      3\u001b[0m y_pred[:,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m][np\u001b[39m.\u001b[39mabs(y_pred[:,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      4\u001b[0m y_pred[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][np\u001b[39m.\u001b[39mabs(y_pred[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m0.5\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m         \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m         \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m         \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3809\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[1;32m   3810\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[39m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/studiarbeit/lib/python3.9/site-packages/pandas/core/indexes/base.py:5925\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   5922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5923\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5924\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5925\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), -2)"
     ]
    }
   ],
   "source": [
    "\n",
    "# converting the predictions to certainity of 0 and 1 (May not be a good idea)\n",
    "y_pred[:,-2][np.abs(y_pred[:,-2]) < 0.5] = 0\n",
    "y_pred[:,-2][np.abs(y_pred[:,-2]) > 0.5] = 1\n",
    "y_pred[:,-1][np.abs(y_pred[:,-1]) < 0.5] = 0\n",
    "y_pred[:,-1][np.abs(y_pred[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred, columns=y_train.columns)\n",
    "y1_pred_inverse = scaler_coords1.inverse_transform(y_pred[['x1', 'y1','x2', 'y2']])\n",
    "y_pred[['x1', 'y1','x2', 'y2']] = pd.DataFrame(y1_pred_inverse,columns=['x1', 'y1','x2', 'y2'])\n",
    "mse = score_mse(y_test, y_pred)\n",
    "y_pred.loc[y_pred['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "y_pred.loc[y_pred['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "\n",
    "y_test.loc[y_test['leak_2'] == 0.0, 'x2'] =  'NaN'\n",
    "y_test.loc[y_test['leak_2'] == 0.0, 'y2'] =  'NaN'\n",
    "pd.concat([y_pred, y_test.reset_index().drop(columns='sample_number')], axis=1).to_csv(model_path+'predictions_nn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
