{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.1\n",
      "# GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1 - With two output layers and 2 different loss function for leakageness and coordinates. Unable to do hyperparameter tuning since so far i couldnt figure\n",
    "# out how to do hyper parameter tuning on functional api\n",
    "# Hence i tried to implement sequential api with 6 outputs and 2 different loss function and do hyper parameter tuning. However i am not able to give 2 loss function \n",
    "# in that case. This experiment of giving two different loss function for each output neuron of neural network is being given in custom_loss.ipynb. \n",
    "\n",
    "# Experiment 2 - 1 output layer with 1 loss function - mse. and do hyper parameter tuning.\n",
    "# Experiment 3 - Use the idea of masking for the case where when we have to only 1 leakage and how to deal with x2 and y2 in that case.\n",
    "from utils.data_preprocess import load_data, load_single_leakage_model_data\n",
    "from utils.module import model_eval, hyper_model, model_comparison, linear_regression, numpy_to_tensor, benchmark_linear_model\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_multi.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.full_load(ymlfile)\n",
    "\n",
    "\n",
    "single_leakage, two_leakage = load_data(total_samples = cfg['experiment']['total_samples'])\n",
    "two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "data['x2'] = data['x2'].replace(np.nan, 0)\n",
    "data['y2'] = data['y2'].replace(np.nan, 0)\n",
    "# Not sure if 0 is good enough or try generating a random number\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual', 'tot_residual_flow', \n",
    "       'total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2']]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', 'leak_1', 'leak_2'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "y1_train = y_train[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_train = y_train[['leak_1', 'leak_2']]\n",
    "y1_test = y_test[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_test = y_test[['leak_1', 'leak_2']]\n",
    "y1_val = y_val[['x1', 'y1', 'x2', 'y2']]\n",
    "y2_val = y_val[['leak_1', 'leak_2']]\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "# scaler_coords2 = StandardScaler()\n",
    "# y2_train = scaler_coords2.fit_transform(y2_train)\n",
    "# y2_test = scaler_coords2.fit_transform(y2_test)\n",
    "# y2_val = scaler_coords2.transform(y2_val)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = {\n",
    "    \"y1\" : y1_train,\n",
    "    \"y2\" : y2_train\n",
    "}\n",
    "\n",
    "y_val = {\n",
    "    \"y1\" : y1_val,\n",
    "    \"y2\" : y2_val\n",
    "}\n",
    "\n",
    "y_test = {\n",
    "    \"y1\" : y1_test,\n",
    "    \"y2\" : y2_test\n",
    "}\n",
    "\n",
    "losses = {\n",
    "\t\"y1\": \"mse\",\n",
    "\t\"y2\": 'binary_crossentropy'\n",
    "    # \"y2\" : 'mse'\n",
    "    }\n",
    "\n",
    "metrics = {\n",
    "    \"y1\": 'mae',\n",
    "    \"y2\": 'mae'\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def base_model(inputs):\n",
    "#     # add the sequential layers here\n",
    "#     x= Dense(128, activation='relu', kernel_initializer='he_uniform')(inputs)\n",
    "#     # x= Dense(128, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "#     return x\n",
    "\n",
    "# def final_model(inputs):\n",
    "#     x = base_model(inputs)\n",
    "#     y1 = Dense(units=4, name='y1')(x)\n",
    "#     y2 = Dense(units =2, name = 'y2')(x)\n",
    "#     model = Model(inputs=inputs, outputs = [y1, y2])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# inputs = tf.keras.layers.Input(shape=(11,))\n",
    "# model = final_model(inputs)\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss=losses, loss_weights=lossWeights,\n",
    "#             metrics = metrics)\n",
    "\n",
    "# history = model.fit(X_train, y_train, validation_data = (X_val, y_val), verbose = 1, epochs=100, shuffle = True)\n",
    "# model_evaluate, y_pred = model_eval(model, X_test, y_test, X_train, y_train, X_val, y_val)\n",
    "# coords = np.concatenate([y_pred[0], y_test['y1']], axis=1)\n",
    "# presence = np.concatenate([y_pred[1], y_test['y2']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 59 Complete [00h 00m 30s]\n",
      "val_loss: 0.009099966358121974\n",
      "\n",
      "Best val_loss So Far: 0.00012803264756660172\n",
      "Total elapsed time: 00h 10m 12s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best Hyperparameters: <keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x7fc3507398e0>\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 3s 18ms/step - loss: 4.5419e-04 - y1_loss: 0.8746 - y2_loss: 2.7783 - y1_mae: 0.7373 - y2_mae: 1.8312 - val_loss: 5.2022e-04 - val_y1_loss: 0.7455 - val_y2_loss: 3.4384 - val_y1_mae: 0.6822 - val_y2_mae: 3.2940\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 4.6442e-04 - y1_loss: 0.7066 - y2_loss: 3.0286 - y1_mae: 0.6805 - y2_mae: 4.4172 - val_loss: 5.7416e-04 - val_y1_loss: 0.6511 - val_y2_loss: 3.9667 - val_y1_mae: 0.6519 - val_y2_mae: 4.4355\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.1100e-04 - y1_loss: 0.6898 - y2_loss: 3.4201 - y1_mae: 0.6734 - y2_mae: 5.3885 - val_loss: 6.8246e-04 - val_y1_loss: 0.6643 - val_y2_loss: 4.8246 - val_y1_mae: 0.6523 - val_y2_mae: 5.0200\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 5.8860e-04 - y1_loss: 0.6994 - y2_loss: 4.0346 - y1_mae: 0.6813 - y2_mae: 5.4195 - val_loss: 6.6166e-04 - val_y1_loss: 0.6234 - val_y2_loss: 4.6982 - val_y1_mae: 0.6363 - val_y2_mae: 6.9950\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 5.9068e-04 - y1_loss: 0.6850 - y2_loss: 4.0657 - y1_mae: 0.6717 - y2_mae: 7.6842 - val_loss: 6.5185e-04 - val_y1_loss: 0.6210 - val_y2_loss: 4.6216 - val_y1_mae: 0.6295 - val_y2_mae: 6.9647\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6285e-04 - y1_loss: 0.6832 - y2_loss: 3.8437 - y1_mae: 0.6730 - y2_mae: 7.7069 - val_loss: 6.3978e-04 - val_y1_loss: 0.6203 - val_y2_loss: 4.5253 - val_y1_mae: 0.6285 - val_y2_mae: 8.2644\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6610e-04 - y1_loss: 0.6842 - y2_loss: 3.8688 - y1_mae: 0.6720 - y2_mae: 9.2543 - val_loss: 6.7135e-04 - val_y1_loss: 0.6346 - val_y2_loss: 4.7649 - val_y1_mae: 0.6383 - val_y2_mae: 9.4450\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.9267e-04 - y1_loss: 0.6863 - y2_loss: 4.0804 - y1_mae: 0.6719 - y2_mae: 10.3351 - val_loss: 6.6353e-04 - val_y1_loss: 0.6242 - val_y2_loss: 4.7124 - val_y1_mae: 0.6336 - val_y2_mae: 9.0859\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.7527e-04 - y1_loss: 0.6870 - y2_loss: 3.9398 - y1_mae: 0.6774 - y2_mae: 9.3832 - val_loss: 6.7068e-04 - val_y1_loss: 0.6234 - val_y2_loss: 4.7707 - val_y1_mae: 0.6330 - val_y2_mae: 7.9681\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.8015e-04 - y1_loss: 0.6820 - y2_loss: 3.9840 - y1_mae: 0.6717 - y2_mae: 8.0438 - val_loss: 6.6916e-04 - val_y1_loss: 0.6621 - val_y2_loss: 4.7198 - val_y1_mae: 0.6416 - val_y2_mae: 7.3495\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.7564e-04 - y1_loss: 0.6825 - y2_loss: 3.9472 - y1_mae: 0.6717 - y2_mae: 7.5133 - val_loss: 6.6435e-04 - val_y1_loss: 0.6248 - val_y2_loss: 4.7185 - val_y1_mae: 0.6372 - val_y2_mae: 6.7689\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.5788e-04 - y1_loss: 0.6821 - y2_loss: 3.8048 - y1_mae: 0.6735 - y2_mae: 4.8859 - val_loss: 6.0319e-04 - val_y1_loss: 0.6294 - val_y2_loss: 4.2219 - val_y1_mae: 0.6504 - val_y2_mae: 5.8888\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.5919e-04 - y1_loss: 0.6854 - y2_loss: 3.8120 - y1_mae: 0.6779 - y2_mae: 7.9667 - val_loss: 6.2938e-04 - val_y1_loss: 0.6242 - val_y2_loss: 4.4378 - val_y1_mae: 0.6364 - val_y2_mae: 8.3637\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.4729e-04 - y1_loss: 0.6884 - y2_loss: 3.7133 - y1_mae: 0.6724 - y2_mae: 9.9288 - val_loss: 6.0705e-04 - val_y1_loss: 0.6752 - val_y2_loss: 4.2072 - val_y1_mae: 0.6982 - val_y2_mae: 11.4437\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.4428e-04 - y1_loss: 0.6950 - y2_loss: 3.6826 - y1_mae: 0.6834 - y2_mae: 13.1756 - val_loss: 6.3518e-04 - val_y1_loss: 0.6185 - val_y2_loss: 4.4901 - val_y1_mae: 0.6371 - val_y2_mae: 13.4784\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.4570e-04 - y1_loss: 0.6845 - y2_loss: 3.7045 - y1_mae: 0.6736 - y2_mae: 14.5154 - val_loss: 6.5314e-04 - val_y1_loss: 0.6810 - val_y2_loss: 4.5720 - val_y1_mae: 0.7077 - val_y2_mae: 14.3146\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.2589e-04 - y1_loss: 0.6878 - y2_loss: 3.5418 - y1_mae: 0.6756 - y2_mae: 15.9647 - val_loss: 5.9622e-04 - val_y1_loss: 0.6178 - val_y2_loss: 4.1774 - val_y1_mae: 0.6348 - val_y2_mae: 16.3319\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.5952e-04 - y1_loss: 0.6847 - y2_loss: 3.8154 - y1_mae: 0.6741 - y2_mae: 17.0161 - val_loss: 6.4130e-04 - val_y1_loss: 0.6239 - val_y2_loss: 4.5340 - val_y1_mae: 0.6446 - val_y2_mae: 16.3837\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.5889e-04 - y1_loss: 0.6844 - y2_loss: 3.8107 - y1_mae: 0.6743 - y2_mae: 17.0762 - val_loss: 6.3505e-04 - val_y1_loss: 0.6161 - val_y2_loss: 4.4914 - val_y1_mae: 0.6310 - val_y2_mae: 16.3901\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.6067e-04 - y1_loss: 0.6846 - y2_loss: 3.8247 - y1_mae: 0.6751 - y2_mae: 17.0786 - val_loss: 6.3900e-04 - val_y1_loss: 0.6402 - val_y2_loss: 4.4991 - val_y1_mae: 0.6367 - val_y2_mae: 16.3947\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.5651e-04 - y1_loss: 0.6823 - y2_loss: 3.7936 - y1_mae: 0.6729 - y2_mae: 16.9541 - val_loss: 6.4562e-04 - val_y1_loss: 0.6142 - val_y2_loss: 4.5784 - val_y1_mae: 0.6274 - val_y2_mae: 15.9407\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.4465e-04 - y1_loss: 0.6827 - y2_loss: 3.6978 - y1_mae: 0.6717 - y2_mae: 16.5847 - val_loss: 5.9433e-04 - val_y1_loss: 0.6126 - val_y2_loss: 4.1674 - val_y1_mae: 0.6270 - val_y2_mae: 16.2374\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.5500e-04 - y1_loss: 0.6806 - y2_loss: 3.7831 - y1_mae: 0.6715 - y2_mae: 16.7799 - val_loss: 6.5364e-04 - val_y1_loss: 0.6424 - val_y2_loss: 4.6148 - val_y1_mae: 0.6469 - val_y2_mae: 16.0348\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.5868e-04 - y1_loss: 0.6856 - y2_loss: 3.8078 - y1_mae: 0.6714 - y2_mae: 16.9479 - val_loss: 6.5551e-04 - val_y1_loss: 0.6146 - val_y2_loss: 4.6575 - val_y1_mae: 0.6206 - val_y2_mae: 16.3224\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.6230e-04 - y1_loss: 0.6803 - y2_loss: 3.8422 - y1_mae: 0.6707 - y2_mae: 17.1229 - val_loss: 6.5582e-04 - val_y1_loss: 0.6170 - val_y2_loss: 4.6575 - val_y1_mae: 0.6218 - val_y2_mae: 16.3504\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6256e-04 - y1_loss: 0.6821 - y2_loss: 3.8424 - y1_mae: 0.6713 - y2_mae: 17.1138 - val_loss: 6.6933e-04 - val_y1_loss: 0.7257 - val_y2_loss: 4.6575 - val_y1_mae: 0.6876 - val_y2_mae: 16.3752\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 5.7114e-04 - y1_loss: 0.6946 - y2_loss: 3.8990 - y1_mae: 0.6808 - y2_mae: 17.2111 - val_loss: 6.5064e-04 - val_y1_loss: 0.6179 - val_y2_loss: 4.6151 - val_y1_mae: 0.6337 - val_y2_mae: 16.5271\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.7057e-04 - y1_loss: 0.6800 - y2_loss: 3.9089 - y1_mae: 0.6713 - y2_mae: 17.5862 - val_loss: 6.4463e-04 - val_y1_loss: 0.6129 - val_y2_loss: 4.5718 - val_y1_mae: 0.6353 - val_y2_mae: 16.8433\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.6640e-04 - y1_loss: 0.6795 - y2_loss: 3.8760 - y1_mae: 0.6731 - y2_mae: 17.8444 - val_loss: 6.4454e-04 - val_y1_loss: 0.6175 - val_y2_loss: 4.5664 - val_y1_mae: 0.6251 - val_y2_mae: 17.0277\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.7214e-04 - y1_loss: 0.6802 - y2_loss: 3.9214 - y1_mae: 0.6712 - y2_mae: 17.9490 - val_loss: 6.4431e-04 - val_y1_loss: 0.6145 - val_y2_loss: 4.5675 - val_y1_mae: 0.6282 - val_y2_mae: 16.7536\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6540e-04 - y1_loss: 0.6810 - y2_loss: 3.8664 - y1_mae: 0.6724 - y2_mae: 16.5942 - val_loss: 6.5668e-04 - val_y1_loss: 0.6240 - val_y2_loss: 4.6575 - val_y1_mae: 0.6275 - val_y2_mae: 15.5293\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6136e-04 - y1_loss: 0.6817 - y2_loss: 3.8333 - y1_mae: 0.6733 - y2_mae: 15.4343 - val_loss: 6.5570e-04 - val_y1_loss: 0.6161 - val_y2_loss: 4.6575 - val_y1_mae: 0.6334 - val_y2_mae: 14.7042\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.5694e-04 - y1_loss: 0.6804 - y2_loss: 3.7989 - y1_mae: 0.6702 - y2_mae: 15.0005 - val_loss: 6.2405e-04 - val_y1_loss: 0.6238 - val_y2_loss: 4.3953 - val_y1_mae: 0.6269 - val_y2_mae: 14.7948\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.2403e-04 - y1_loss: 0.6837 - y2_loss: 3.5310 - y1_mae: 0.6743 - y2_mae: 15.3223 - val_loss: 6.3084e-04 - val_y1_loss: 0.6384 - val_y2_loss: 4.4353 - val_y1_mae: 0.6571 - val_y2_mae: 14.9845\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.4949e-04 - y1_loss: 0.6826 - y2_loss: 3.7368 - y1_mae: 0.6725 - y2_mae: 15.8657 - val_loss: 6.3319e-04 - val_y1_loss: 0.6134 - val_y2_loss: 4.4792 - val_y1_mae: 0.6328 - val_y2_mae: 15.8217\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6514e-04 - y1_loss: 0.6794 - y2_loss: 3.8658 - y1_mae: 0.6737 - y2_mae: 17.0206 - val_loss: 6.4281e-04 - val_y1_loss: 0.6475 - val_y2_loss: 4.5224 - val_y1_mae: 0.6401 - val_y2_mae: 16.3966\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6067e-04 - y1_loss: 0.6853 - y2_loss: 3.8241 - y1_mae: 0.6731 - y2_mae: 18.0151 - val_loss: 6.1493e-04 - val_y1_loss: 0.6435 - val_y2_loss: 4.3023 - val_y1_mae: 0.6629 - val_y2_mae: 18.0334\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.4158e-04 - y1_loss: 0.6878 - y2_loss: 3.6679 - y1_mae: 0.6762 - y2_mae: 19.8096 - val_loss: 5.1723e-04 - val_y1_loss: 0.6149 - val_y2_loss: 3.5451 - val_y1_mae: 0.6339 - val_y2_mae: 18.2336\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.4912e-04 - y1_loss: 0.6811 - y2_loss: 2.9311 - y1_mae: 0.6736 - y2_mae: 19.1643 - val_loss: 4.8754e-04 - val_y1_loss: 0.7164 - val_y2_loss: 3.2048 - val_y1_mae: 0.6735 - val_y2_mae: 17.5748\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.0673e-04 - y1_loss: 0.6883 - y2_loss: 3.3872 - y1_mae: 0.6774 - y2_mae: 19.4141 - val_loss: 5.3683e-04 - val_y1_loss: 0.6370 - val_y2_loss: 3.6806 - val_y1_mae: 0.6357 - val_y2_mae: 19.7766\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.1404e-04 - y1_loss: 0.6817 - y2_loss: 3.4526 - y1_mae: 0.6722 - y2_mae: 21.8576 - val_loss: 5.2504e-04 - val_y1_loss: 0.6494 - val_y2_loss: 3.5734 - val_y1_mae: 0.6538 - val_y2_mae: 21.1040\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.0532e-04 - y1_loss: 0.6862 - y2_loss: 3.3780 - y1_mae: 0.6744 - y2_mae: 22.1883 - val_loss: 5.1371e-04 - val_y1_loss: 0.6319 - val_y2_loss: 3.4997 - val_y1_mae: 0.6405 - val_y2_mae: 20.4178\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.6985e-04 - y1_loss: 0.6854 - y2_loss: 3.0935 - y1_mae: 0.6714 - y2_mae: 21.2175 - val_loss: 4.9717e-04 - val_y1_loss: 0.6589 - val_y2_loss: 3.3397 - val_y1_mae: 0.6361 - val_y2_mae: 19.7452\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.6336e-04 - y1_loss: 0.6835 - y2_loss: 3.0432 - y1_mae: 0.6732 - y2_mae: 20.9458 - val_loss: 4.7833e-04 - val_y1_loss: 0.6395 - val_y2_loss: 3.2076 - val_y1_mae: 0.6323 - val_y2_mae: 19.5747\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 4.4378e-04 - y1_loss: 0.6832 - y2_loss: 2.8861 - y1_mae: 0.6719 - y2_mae: 20.7351 - val_loss: 4.5423e-04 - val_y1_loss: 0.6291 - val_y2_loss: 3.0241 - val_y1_mae: 0.6406 - val_y2_mae: 19.2614\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.2406e-04 - y1_loss: 0.6887 - y2_loss: 2.7219 - y1_mae: 0.6758 - y2_mae: 20.4463 - val_loss: 4.5240e-04 - val_y1_loss: 0.6133 - val_y2_loss: 3.0252 - val_y1_mae: 0.6247 - val_y2_mae: 19.2503\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 4.1894e-04 - y1_loss: 0.6813 - y2_loss: 2.6882 - y1_mae: 0.6719 - y2_mae: 20.4313 - val_loss: 4.5614e-04 - val_y1_loss: 0.6939 - val_y2_loss: 2.9748 - val_y1_mae: 0.6777 - val_y2_mae: 19.2409\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 4.1816e-04 - y1_loss: 0.6873 - y2_loss: 2.6759 - y1_mae: 0.6750 - y2_mae: 20.4157 - val_loss: 4.4498e-04 - val_y1_loss: 0.6337 - val_y2_loss: 2.9451 - val_y1_mae: 0.6411 - val_y2_mae: 18.9658\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 4.0625e-04 - y1_loss: 0.6842 - y2_loss: 2.5832 - y1_mae: 0.6719 - y2_mae: 20.0381 - val_loss: 4.3885e-04 - val_y1_loss: 0.6434 - val_y2_loss: 2.8862 - val_y1_mae: 0.6366 - val_y2_mae: 18.7221\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.9754e-04 - y1_loss: 0.6825 - y2_loss: 2.5148 - y1_mae: 0.6734 - y2_mae: 19.7887 - val_loss: 4.2491e-04 - val_y1_loss: 0.6234 - val_y2_loss: 2.7940 - val_y1_mae: 0.6332 - val_y2_mae: 18.3423\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.9130e-04 - y1_loss: 0.6798 - y2_loss: 2.4673 - y1_mae: 0.6726 - y2_mae: 19.5391 - val_loss: 4.2383e-04 - val_y1_loss: 0.6158 - val_y2_loss: 2.7930 - val_y1_mae: 0.6226 - val_y2_mae: 18.2481\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.8966e-04 - y1_loss: 0.6783 - y2_loss: 2.4557 - y1_mae: 0.6708 - y2_mae: 19.4777 - val_loss: 4.1234e-04 - val_y1_loss: 0.6129 - val_y2_loss: 2.7035 - val_y1_mae: 0.6308 - val_y2_mae: 18.1695\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 4.5338e-04 - y1_loss: 0.6808 - y2_loss: 2.9656 - y1_mae: 0.6706 - y2_mae: 19.0033 - val_loss: 6.5033e-04 - val_y1_loss: 0.6156 - val_y2_loss: 4.6149 - val_y1_mae: 0.6308 - val_y2_mae: 17.1881\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.7085e-04 - y1_loss: 0.6812 - y2_loss: 3.9100 - y1_mae: 0.6732 - y2_mae: 18.2804 - val_loss: 6.5681e-04 - val_y1_loss: 0.6251 - val_y2_loss: 4.6575 - val_y1_mae: 0.6322 - val_y2_mae: 17.2188\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.0527e-04 - y1_loss: 0.6801 - y2_loss: 3.3837 - y1_mae: 0.6725 - y2_mae: 18.3463 - val_loss: 3.5650e-04 - val_y1_loss: 0.6167 - val_y2_loss: 2.2506 - val_y1_mae: 0.6382 - val_y2_mae: 17.3110\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.4528e-04 - y1_loss: 0.6833 - y2_loss: 2.0937 - y1_mae: 0.6754 - y2_mae: 18.4060 - val_loss: 3.6130e-04 - val_y1_loss: 0.7416 - val_y2_loss: 2.1642 - val_y1_mae: 0.6852 - val_y2_mae: 17.2142\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.5051e-04 - y1_loss: 0.7503 - y2_loss: 2.0688 - y1_mae: 0.7118 - y2_mae: 18.2740 - val_loss: 3.4075e-04 - val_y1_loss: 0.6287 - val_y2_loss: 2.1118 - val_y1_mae: 0.6304 - val_y2_mae: 17.1317\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3951e-04 - y1_loss: 0.6868 - y2_loss: 2.0438 - y1_mae: 0.6717 - y2_mae: 18.2340 - val_loss: 3.3375e-04 - val_y1_loss: 0.6193 - val_y2_loss: 2.0650 - val_y1_mae: 0.6356 - val_y2_mae: 17.0961\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3874e-04 - y1_loss: 0.6815 - y2_loss: 2.0430 - y1_mae: 0.6713 - y2_mae: 18.2148 - val_loss: 3.3287e-04 - val_y1_loss: 0.6123 - val_y2_loss: 2.0649 - val_y1_mae: 0.6254 - val_y2_mae: 17.0822\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 3.3826e-04 - y1_loss: 0.6779 - y2_loss: 2.0427 - y1_mae: 0.6695 - y2_mae: 18.2000 - val_loss: 3.3524e-04 - val_y1_loss: 0.6327 - val_y2_loss: 2.0635 - val_y1_mae: 0.6400 - val_y2_mae: 17.0805\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3856e-04 - y1_loss: 0.6811 - y2_loss: 2.0419 - y1_mae: 0.6717 - y2_mae: 18.1890 - val_loss: 3.3290e-04 - val_y1_loss: 0.6143 - val_y2_loss: 2.0631 - val_y1_mae: 0.6345 - val_y2_mae: 17.0696\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 3.3816e-04 - y1_loss: 0.6779 - y2_loss: 2.0418 - y1_mae: 0.6715 - y2_mae: 18.1702 - val_loss: 3.4379e-04 - val_y1_loss: 0.6999 - val_y2_loss: 2.0651 - val_y1_mae: 0.6656 - val_y2_mae: 17.0351\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3915e-04 - y1_loss: 0.6860 - y2_loss: 2.0417 - y1_mae: 0.6773 - y2_mae: 18.1524 - val_loss: 3.3402e-04 - val_y1_loss: 0.6229 - val_y2_loss: 2.0635 - val_y1_mae: 0.6434 - val_y2_mae: 17.0537\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3814e-04 - y1_loss: 0.6782 - y2_loss: 2.0414 - y1_mae: 0.6728 - y2_mae: 18.1388 - val_loss: 3.3411e-04 - val_y1_loss: 0.6234 - val_y2_loss: 2.0638 - val_y1_mae: 0.6405 - val_y2_mae: 17.0404\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3819e-04 - y1_loss: 0.6787 - y2_loss: 2.0413 - y1_mae: 0.6724 - y2_mae: 18.1271 - val_loss: 3.3279e-04 - val_y1_loss: 0.6143 - val_y2_loss: 2.0623 - val_y1_mae: 0.6248 - val_y2_mae: 17.0152\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3788e-04 - y1_loss: 0.6765 - y2_loss: 2.0410 - y1_mae: 0.6695 - y2_mae: 18.1145 - val_loss: 3.3330e-04 - val_y1_loss: 0.6182 - val_y2_loss: 2.0625 - val_y1_mae: 0.6395 - val_y2_mae: 17.0152\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3789e-04 - y1_loss: 0.6766 - y2_loss: 2.0409 - y1_mae: 0.6700 - y2_mae: 18.1104 - val_loss: 3.3743e-04 - val_y1_loss: 0.6518 - val_y2_loss: 2.0621 - val_y1_mae: 0.6378 - val_y2_mae: 17.0048\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3879e-04 - y1_loss: 0.6839 - y2_loss: 2.0409 - y1_mae: 0.6764 - y2_mae: 18.1025 - val_loss: 3.3311e-04 - val_y1_loss: 0.6170 - val_y2_loss: 2.0621 - val_y1_mae: 0.6230 - val_y2_mae: 17.0067\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3817e-04 - y1_loss: 0.6791 - y2_loss: 2.0408 - y1_mae: 0.6717 - y2_mae: 18.0977 - val_loss: 3.3363e-04 - val_y1_loss: 0.6214 - val_y2_loss: 2.0619 - val_y1_mae: 0.6387 - val_y2_mae: 16.9931\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3816e-04 - y1_loss: 0.6789 - y2_loss: 2.0408 - y1_mae: 0.6720 - y2_mae: 18.0900 - val_loss: 3.3351e-04 - val_y1_loss: 0.6206 - val_y2_loss: 2.0618 - val_y1_mae: 0.6252 - val_y2_mae: 16.9908\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3830e-04 - y1_loss: 0.6802 - y2_loss: 2.0407 - y1_mae: 0.6739 - y2_mae: 18.0848 - val_loss: 3.3645e-04 - val_y1_loss: 0.6441 - val_y2_loss: 2.0619 - val_y1_mae: 0.6466 - val_y2_mae: 16.9807\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3826e-04 - y1_loss: 0.6799 - y2_loss: 2.0406 - y1_mae: 0.6701 - y2_mae: 18.0683 - val_loss: 3.3365e-04 - val_y1_loss: 0.6215 - val_y2_loss: 2.0619 - val_y1_mae: 0.6416 - val_y2_mae: 16.9567\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 3.3863e-04 - y1_loss: 0.6829 - y2_loss: 2.0406 - y1_mae: 0.6764 - y2_mae: 18.0598 - val_loss: 3.3311e-04 - val_y1_loss: 0.6175 - val_y2_loss: 2.0616 - val_y1_mae: 0.6320 - val_y2_mae: 16.9642\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3782e-04 - y1_loss: 0.6764 - y2_loss: 2.0406 - y1_mae: 0.6694 - y2_mae: 18.0604 - val_loss: 3.3304e-04 - val_y1_loss: 0.6149 - val_y2_loss: 2.0637 - val_y1_mae: 0.6342 - val_y2_mae: 16.9687\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3776e-04 - y1_loss: 0.6758 - y2_loss: 2.0407 - y1_mae: 0.6704 - y2_mae: 18.0570 - val_loss: 3.3385e-04 - val_y1_loss: 0.6235 - val_y2_loss: 2.0617 - val_y1_mae: 0.6462 - val_y2_mae: 16.9460\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3779e-04 - y1_loss: 0.6763 - y2_loss: 2.0405 - y1_mae: 0.6703 - y2_mae: 18.0419 - val_loss: 3.3258e-04 - val_y1_loss: 0.6134 - val_y2_loss: 2.0615 - val_y1_mae: 0.6310 - val_y2_mae: 16.9354\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3779e-04 - y1_loss: 0.6763 - y2_loss: 2.0404 - y1_mae: 0.6724 - y2_mae: 18.0273 - val_loss: 3.3249e-04 - val_y1_loss: 0.6116 - val_y2_loss: 2.0625 - val_y1_mae: 0.6221 - val_y2_mae: 16.9297\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3824e-04 - y1_loss: 0.6799 - y2_loss: 2.0405 - y1_mae: 0.6737 - y2_mae: 18.0196 - val_loss: 3.3378e-04 - val_y1_loss: 0.6230 - val_y2_loss: 2.0615 - val_y1_mae: 0.6364 - val_y2_mae: 16.9100\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3805e-04 - y1_loss: 0.6784 - y2_loss: 2.0405 - y1_mae: 0.6694 - y2_mae: 18.0044 - val_loss: 3.3309e-04 - val_y1_loss: 0.6176 - val_y2_loss: 2.0614 - val_y1_mae: 0.6365 - val_y2_mae: 16.8862\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3783e-04 - y1_loss: 0.6767 - y2_loss: 2.0404 - y1_mae: 0.6717 - y2_mae: 17.9859 - val_loss: 3.3411e-04 - val_y1_loss: 0.6259 - val_y2_loss: 2.0613 - val_y1_mae: 0.6501 - val_y2_mae: 16.8752\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3774e-04 - y1_loss: 0.6760 - y2_loss: 2.0404 - y1_mae: 0.6715 - y2_mae: 17.9745 - val_loss: 3.3230e-04 - val_y1_loss: 0.6109 - val_y2_loss: 2.0617 - val_y1_mae: 0.6263 - val_y2_mae: 16.8724\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3780e-04 - y1_loss: 0.6765 - y2_loss: 2.0404 - y1_mae: 0.6698 - y2_mae: 17.9672 - val_loss: 3.4297e-04 - val_y1_loss: 0.6966 - val_y2_loss: 2.0618 - val_y1_mae: 0.6635 - val_y2_mae: 16.8636\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3958e-04 - y1_loss: 0.6907 - y2_loss: 2.0404 - y1_mae: 0.6794 - y2_mae: 17.9783 - val_loss: 3.3270e-04 - val_y1_loss: 0.6133 - val_y2_loss: 2.0625 - val_y1_mae: 0.6235 - val_y2_mae: 16.8619\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3776e-04 - y1_loss: 0.6761 - y2_loss: 2.0404 - y1_mae: 0.6704 - y2_mae: 17.9837 - val_loss: 3.3244e-04 - val_y1_loss: 0.6122 - val_y2_loss: 2.0615 - val_y1_mae: 0.6314 - val_y2_mae: 16.8672\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3794e-04 - y1_loss: 0.6776 - y2_loss: 2.0403 - y1_mae: 0.6717 - y2_mae: 17.9715 - val_loss: 3.3508e-04 - val_y1_loss: 0.6336 - val_y2_loss: 2.0614 - val_y1_mae: 0.6370 - val_y2_mae: 16.8593\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3817e-04 - y1_loss: 0.6795 - y2_loss: 2.0403 - y1_mae: 0.6707 - y2_mae: 17.9743 - val_loss: 3.3568e-04 - val_y1_loss: 0.6387 - val_y2_loss: 2.0610 - val_y1_mae: 0.6559 - val_y2_mae: 16.8582\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3857e-04 - y1_loss: 0.6828 - y2_loss: 2.0403 - y1_mae: 0.6743 - y2_mae: 17.9757 - val_loss: 3.3370e-04 - val_y1_loss: 0.6216 - val_y2_loss: 2.0623 - val_y1_mae: 0.6417 - val_y2_mae: 16.8757\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 3.3790e-04 - y1_loss: 0.6775 - y2_loss: 2.0402 - y1_mae: 0.6716 - y2_mae: 17.9716 - val_loss: 3.3863e-04 - val_y1_loss: 0.6624 - val_y2_loss: 2.0612 - val_y1_mae: 0.6537 - val_y2_mae: 16.8552\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3851e-04 - y1_loss: 0.6823 - y2_loss: 2.0402 - y1_mae: 0.6753 - y2_mae: 17.9498 - val_loss: 3.3384e-04 - val_y1_loss: 0.6241 - val_y2_loss: 2.0609 - val_y1_mae: 0.6211 - val_y2_mae: 16.8282\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3782e-04 - y1_loss: 0.6768 - y2_loss: 2.0401 - y1_mae: 0.6709 - y2_mae: 17.9435 - val_loss: 3.3286e-04 - val_y1_loss: 0.6162 - val_y2_loss: 2.0609 - val_y1_mae: 0.6384 - val_y2_mae: 16.8167\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3790e-04 - y1_loss: 0.6774 - y2_loss: 2.0403 - y1_mae: 0.6715 - y2_mae: 17.9286 - val_loss: 3.3310e-04 - val_y1_loss: 0.6182 - val_y2_loss: 2.0609 - val_y1_mae: 0.6334 - val_y2_mae: 16.7970\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3781e-04 - y1_loss: 0.6767 - y2_loss: 2.0402 - y1_mae: 0.6701 - y2_mae: 17.9147 - val_loss: 3.3331e-04 - val_y1_loss: 0.6197 - val_y2_loss: 2.0610 - val_y1_mae: 0.6392 - val_y2_mae: 16.7725\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3805e-04 - y1_loss: 0.6787 - y2_loss: 2.0402 - y1_mae: 0.6709 - y2_mae: 17.8891 - val_loss: 3.3709e-04 - val_y1_loss: 0.6503 - val_y2_loss: 2.0609 - val_y1_mae: 0.6730 - val_y2_mae: 16.7391\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3825e-04 - y1_loss: 0.6803 - y2_loss: 2.0401 - y1_mae: 0.6755 - y2_mae: 17.8717 - val_loss: 3.3422e-04 - val_y1_loss: 0.6270 - val_y2_loss: 2.0610 - val_y1_mae: 0.6378 - val_y2_mae: 16.7451\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 3.3774e-04 - y1_loss: 0.6762 - y2_loss: 2.0402 - y1_mae: 0.6709 - y2_mae: 17.8609 - val_loss: 3.3291e-04 - val_y1_loss: 0.6164 - val_y2_loss: 2.0612 - val_y1_mae: 0.6213 - val_y2_mae: 16.7273\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3781e-04 - y1_loss: 0.6769 - y2_loss: 2.0401 - y1_mae: 0.6708 - y2_mae: 17.8627 - val_loss: 3.3255e-04 - val_y1_loss: 0.6138 - val_y2_loss: 2.0609 - val_y1_mae: 0.6278 - val_y2_mae: 16.7318\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3785e-04 - y1_loss: 0.6771 - y2_loss: 2.0401 - y1_mae: 0.6726 - y2_mae: 17.8626 - val_loss: 3.3276e-04 - val_y1_loss: 0.6153 - val_y2_loss: 2.0610 - val_y1_mae: 0.6341 - val_y2_mae: 16.7178\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3772e-04 - y1_loss: 0.6762 - y2_loss: 2.0401 - y1_mae: 0.6698 - y2_mae: 17.8520 - val_loss: 3.3320e-04 - val_y1_loss: 0.6187 - val_y2_loss: 2.0612 - val_y1_mae: 0.6426 - val_y2_mae: 16.7112\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3775e-04 - y1_loss: 0.6763 - y2_loss: 2.0401 - y1_mae: 0.6715 - y2_mae: 17.8493 - val_loss: 3.3216e-04 - val_y1_loss: 0.6108 - val_y2_loss: 2.0607 - val_y1_mae: 0.6226 - val_y2_mae: 16.6999\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.3756e-04 - y1_loss: 0.6748 - y2_loss: 2.0401 - y1_mae: 0.6699 - y2_mae: 17.8448 - val_loss: 3.3554e-04 - val_y1_loss: 0.6380 - val_y2_loss: 2.0607 - val_y1_mae: 0.6402 - val_y2_mae: 16.6880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc31c209c70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner import HyperModel, Hyperband\n",
    "\n",
    "EPOCHS = 100\n",
    "# Define the multi-task HyperModel\n",
    "class MultiTaskHyperModel(HyperModel):\n",
    "\n",
    "    def build(self, hp):\n",
    "        inputs = keras.Input(shape=(10,))\n",
    "        shared_layer = inputs\n",
    "        for i in range(hp.Int('num_layers', 1, 5)):\n",
    "            shared_layer = layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=128, step=32),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\", \"elu\"]),\n",
    "                kernel_initializer='he_uniform'\n",
    "            )(shared_layer)\n",
    "\n",
    "\n",
    "        task_layer1 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 5)):\n",
    "            task_layer1 = layers.Dense(units=hp.Choice(f'task_{i}_layer_{j}_neurons', values=[4, 8, 16]), activation='relu')(task_layer1)\n",
    "        y1 = layers.Dense(1, name='y1')(task_layer1)\n",
    "\n",
    "        task_layer2 = shared_layer\n",
    "        for j in range(hp.Int(f'task_{i}_num_layers', 0, 5)):\n",
    "            task_layer2 = layers.Dense(units=hp.Choice(f'task_{i}_layer_{j}_neurons', values=[4, 8, 16]), activation='relu')(task_layer2)\n",
    "        y2 = layers.Dense(1, name='y2')(task_layer2)\n",
    "\n",
    "        outputs = [y1, y2]\n",
    "\n",
    "        loss1_weight = hp.Float(\"loss1_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "        loss1_weight = hp.Float(\"loss2_weight\", min_value=1e-4, max_value=1, sampling=\"log\")\n",
    "\n",
    "        # loss1_weight = hp.Choice('loss1_weight', values=[0.00001, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        # loss2_weight = hp.Choice('loss2_weight', values=[0.00005, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "        lossWeights = {\"y1\": loss1_weight, \"y2\": loss1_weight}\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "        # model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        #             loss=\"mse\",  metrics='mae')\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                    loss=losses, loss_weights=lossWeights,\n",
    "                    metrics = metrics)\n",
    "        return model\n",
    "\n",
    "# Create the multi-task HyperModel\n",
    "multi_task_hypermodel = MultiTaskHyperModel()\n",
    "\n",
    "# Define the Hyperband tuner\n",
    "tuner = Hyperband(\n",
    "    multi_task_hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_epochs=EPOCHS,\n",
    "    factor=5,\n",
    "    directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "    project_name='multi_task_tuning'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(X_train, y_train, validation_data = (X_val, y_val), verbose = 1, epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hps)\n",
    "\n",
    "# Build the best model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "best_model.fit(X_train, y_train, validation_data = (X_val, y_val), verbose = 1, epochs=EPOCHS, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           704         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4)            260         ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4)            260         ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " y1 (Dense)                     (None, 1)            5           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " y2 (Dense)                     (None, 1)            5           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,234\n",
      "Trainable params: 1,234\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 3.4026e-04 - y1_loss: 0.6967 - y2_loss: 2.0400 - y1_mae: 0.6698 - y2_mae: 17.8389\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.3554e-04 - y1_loss: 0.6380 - y2_loss: 2.0607 - y1_mae: 0.6402 - y2_mae: 16.6880\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.4062e-04 - y1_loss: 0.6789 - y2_loss: 2.0607 - y1_mae: 0.6584 - y2_mae: 19.1397\n"
     ]
    }
   ],
   "source": [
    "y_predictions = best_model.predict(X_test)\n",
    "results_train = best_model.evaluate(X_train, y_train, verbose=1)\n",
    "results_val = best_model.evaluate(X_val, y_val, verbose=1)\n",
    "results_test = best_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
