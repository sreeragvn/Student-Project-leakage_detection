{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN 4 outputs and with and without sorting of outputs\n",
    "model_path = 'saved_model/Multi_leak/experiment5/Mask_MTL/'\n",
    "project_name='Multi_leak_experiment5_Mask_MTL_withSwap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 - 1 output layer with 1 loss function - mse. and do hyper parameter tuning.\n",
    "from utils.data_preprocess import load_data\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras_tuner as kt\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from kerastuner import HyperModel, Hyperband\n",
    "from keras import backend as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_leakage, two_leakage = load_data()\n",
    "# two_leakage[\"leak_1\"] = 1\n",
    "two_leakage[\"leak_2\"] = 1\n",
    "\n",
    "# single_leakage[\"leak_1\"] = 1\n",
    "single_leakage[\"leak_2\"] = 0\n",
    "\n",
    "data = pd.concat([single_leakage, two_leakage], axis=0)\n",
    "data['x2'] = data['x2'].replace(np.nan, 0)\n",
    "data['y2'] = data['y2'].replace(np.nan, 0)\n",
    "\n",
    "data = data.drop(columns=['mfc6_residual',\n",
    "       'mfc7_residual', 'mfc8_residual', 'mfc9_residual', 'mfc10_residual',\n",
    "       'mfc1_residual', 'mfc2_residual', 'mfc3_residual', 'mfc4_residual',\n",
    "       'mfc5_residual', 'total flow rate'\n",
    "       ])\n",
    "\n",
    "y = data[['x1', 'y1', 'x2', 'y2', \"leak_2\"]]\n",
    "x = data.drop(['x1', 'y1', 'x2', 'y2', \"leak_2\"], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) \n",
    "\n",
    "\n",
    "# sort x1, y1 and x2, y2. coordinates with lowest x will take the position of x1\n",
    "def coords_swap(y1):\n",
    "    s = y1['x2'] < y1['x1']\n",
    "    y1.loc[s, ['x1','x2']] = y1.loc[s, ['x2','x1']].values\n",
    "    y1.loc[s, ['y1','y2']] = y1.loc[s, ['y2','y1']].values\n",
    "    return y1\n",
    "\n",
    "\n",
    "y1_data = [y_train, y_val, y_test]\n",
    "y1_data_types = ['y_train', 'y_val', 'y_test']\n",
    "for y1_data_types, y1 in zip(y1_data_types, y1_data):\n",
    "    y1_data_types = coords_swap(y1)\n",
    "\n",
    "y1_train = y_train[['x1', 'y1']]\n",
    "y1_test = y_test[['x1', 'y1']]\n",
    "y1_val = y_val[['x1', 'y1']]\n",
    "\n",
    "y2_train = y_train[['x2', 'y2']]\n",
    "y2_test = y_test[['x2', 'y2']]\n",
    "y2_val = y_val[['x2', 'y2']]\n",
    "\n",
    "y3_train = y_train[[\"leak_2\"]]\n",
    "y3_test = y_test[[\"leak_2\"]]\n",
    "y3_val = y_val[[\"leak_2\"]]\n",
    "\n",
    "y1_columns = y1_train.columns\n",
    "y2_columns = y2_train.columns\n",
    "y3_columns = y3_train.columns\n",
    "X_columns = X_train.columns\n",
    "\n",
    "scaler_coords1 = StandardScaler()\n",
    "y1_train = scaler_coords1.fit_transform(y1_train)\n",
    "y1_test = scaler_coords1.transform(y1_test)\n",
    "y1_val = scaler_coords1.transform(y1_val)\n",
    "\n",
    "y1_train = pd.DataFrame(y1_train, columns=y1_columns)\n",
    "y1_test = pd.DataFrame(y1_test, columns=y1_columns)\n",
    "y1_val = pd.DataFrame(y1_val, columns=y1_columns)\n",
    "\n",
    "scaler_coords2 = StandardScaler()\n",
    "y2_train = scaler_coords2.fit_transform(y2_train)\n",
    "y2_test = scaler_coords2.transform(y2_test)\n",
    "y2_val = scaler_coords2.transform(y2_val)\n",
    "\n",
    "y2_train = pd.DataFrame(y2_train, columns=y2_columns)\n",
    "y2_test = pd.DataFrame(y2_test, columns=y2_columns)\n",
    "y2_val = pd.DataFrame(y2_val, columns=y2_columns)\n",
    "\n",
    "scaler_flows = StandardScaler()\n",
    "X_train = scaler_flows.fit_transform(X_train)\n",
    "X_test = scaler_flows.transform(X_test)\n",
    "X_val = scaler_flows.transform(X_val)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X_columns)\n",
    "\n",
    "# y_train = [y1_train, y2_train]\n",
    "# y_val = [y1_val, y2_val]\n",
    "# y_test = [y1_test, y2_test]\n",
    "\n",
    "y3_train = y3_train.reset_index().drop(columns='sample_number')\n",
    "y3_val = y3_val.reset_index().drop(columns='sample_number')\n",
    "y3_test = y3_test.reset_index().drop(columns='sample_number')\n",
    "\n",
    "y_train_all = pd.concat([y1_train, y2_train, y3_train], axis=1)\n",
    "y_test_all = pd.concat([y1_test, y2_test, y3_test], axis=1)\n",
    "y_val_all = pd.concat([y1_val, y2_val, y3_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, y1_train_np, y2_train_np, y3_train_np = X_train.values, y1_train.values, y2_train.values, y3_train.values\n",
    "X_val_np, y1_val_np, y2_val_np, y3_val_np = X_val.values, y1_val.values, y2_val.values, y3_val.values\n",
    "X_test_np, y1_test_np, y2_test_np, y3_test_np = X_test.values, y1_test.values, y2_test.values, y3_test.values\n",
    "\n",
    "# Create TensorFlow datasets from NumPy arrays.\n",
    "batch_size = 32\n",
    "buffer_size = len(X_train)  # Set the buffer size to the number of training examples for full shuffling.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y1_train_np, y2_train_np , y3_train_np))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y1_val_np, y2_val_np, y3_val_np))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_np, y1_test_np, y2_test_np, y3_test_np))\n",
    "\n",
    "# Shuffle, batch, and prefetch the training dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch the validation and test datasets.\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    mask = tf.keras.backend.cast(tf.keras.backend.not_equal(y_true, 0), tf.keras.backend.floatx())\n",
    "    mse = tf.keras.backend.mean(tf.keras.backend.square(y_true - y_pred) * mask)\n",
    "    return mse\n",
    "\n",
    "losses = {\n",
    "\t\"y1\": \"mse\",\n",
    "\t\"y2\": masked_mse,\n",
    "    \"y3\": 'binary_crossentropy'\n",
    "    }\n",
    "\n",
    "metrics = {\n",
    "    \"y1\": 'mae',\n",
    "    \"y2\": 'mae',\n",
    "    \"y3\": 'mae'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2111 Complete [00h 00m 09s]\n",
      "val_loss: 0.06350337713956833\n",
      "\n",
      "Best val_loss So Far: 0.058091480284929276\n",
      "Total elapsed time: 00h 00m 09s\n",
      "\n",
      "Search: Running Trial #2112\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "13                |5                 |num_layers\n",
      "288               |64                |units_0\n",
      "6                 |1                 |task_0_num_layers\n",
      "0.6               |0.6               |loss1_weight\n",
      "0.00088375        |0.0044231         |lr\n",
      "512               |512               |units_1\n",
      "160               |416               |units_2\n",
      "288               |256               |units_3\n",
      "352               |64                |units_4\n",
      "416               |448               |units_5\n",
      "480               |320               |units_6\n",
      "6                 |9                 |task_6_num_layers\n",
      "96                |256               |units_7\n",
      "192               |96                |units_8\n",
      "288               |32                |units_9\n",
      "416               |384               |units_10\n",
      "160               |352               |units_11\n",
      "6                 |1                 |task_11_num_layers\n",
      "8                 |10                |task_2_num_layers\n",
      "120               |88                |task_11_layer_0_neurons\n",
      "8                 |128               |task_11_layer_1_neurons\n",
      "88                |104               |task_11_layer_2_neurons\n",
      "112               |56                |task_11_layer_3_neurons\n",
      "88                |32                |task_11_layer_4_neurons\n",
      "72                |128               |task_11_layer_5_neurons\n",
      "24                |104               |task_11_layer_6_neurons\n",
      "3                 |2                 |task_4_num_layers\n",
      "352               |448               |units_12\n",
      "288               |384               |units_13\n",
      "0                 |5                 |task_13_num_layers\n",
      "64                |16                |task_6_layer_0_neurons\n",
      "72                |32                |task_6_layer_1_neurons\n",
      "32                |32                |task_6_layer_2_neurons\n",
      "16                |80                |task_6_layer_3_neurons\n",
      "48                |8                 |task_6_layer_4_neurons\n",
      "96                |120               |task_6_layer_5_neurons\n",
      "104               |96                |task_6_layer_6_neurons\n",
      "1                 |5                 |task_3_num_layers\n",
      "7                 |4                 |task_5_num_layers\n",
      "256               |256               |units_14\n",
      "4                 |1                 |task_14_num_layers\n",
      "96                |104               |task_5_layer_0_neurons\n",
      "88                |24                |task_5_layer_1_neurons\n",
      "80                |64                |task_5_layer_2_neurons\n",
      "24                |112               |task_5_layer_3_neurons\n",
      "48                |96                |task_5_layer_4_neurons\n",
      "120               |104               |task_5_layer_5_neurons\n",
      "24                |16                |task_5_layer_6_neurons\n",
      "128               |32                |task_13_layer_0_neurons\n",
      "48                |8                 |task_13_layer_1_neurons\n",
      "88                |32                |task_13_layer_2_neurons\n",
      "8                 |88                |task_13_layer_3_neurons\n",
      "5                 |7                 |task_8_num_layers\n",
      "64                |48                |task_14_layer_0_neurons\n",
      "128               |48                |task_14_layer_1_neurons\n",
      "8                 |72                |task_14_layer_2_neurons\n",
      "72                |64                |task_14_layer_3_neurons\n",
      "112               |8                 |task_14_layer_4_neurons\n",
      "112               |56                |task_14_layer_5_neurons\n",
      "104               |112               |task_14_layer_6_neurons\n",
      "48                |32                |task_14_layer_7_neurons\n",
      "3                 |10                |task_7_num_layers\n",
      "0                 |4                 |task_9_num_layers\n",
      "40                |128               |task_3_layer_0_neurons\n",
      "128               |112               |task_5_layer_7_neurons\n",
      "120               |80                |task_5_layer_8_neurons\n",
      "96                |80                |task_5_layer_9_neurons\n",
      "88                |40                |task_11_layer_7_neurons\n",
      "120               |64                |task_11_layer_8_neurons\n",
      "112               |88                |task_2_layer_0_neurons\n",
      "48                |16                |task_2_layer_1_neurons\n",
      "120               |48                |task_2_layer_2_neurons\n",
      "72                |128               |task_2_layer_3_neurons\n",
      "16                |120               |task_2_layer_4_neurons\n",
      "64                |8                 |task_2_layer_5_neurons\n",
      "120               |112               |task_2_layer_6_neurons\n",
      "40                |8                 |task_2_layer_7_neurons\n",
      "72                |32                |task_7_layer_0_neurons\n",
      "64                |96                |task_7_layer_1_neurons\n",
      "104               |80                |task_7_layer_2_neurons\n",
      "8                 |24                |task_7_layer_3_neurons\n",
      "40                |112               |task_7_layer_4_neurons\n",
      "64                |96                |task_0_layer_0_neurons\n",
      "64                |104               |task_0_layer_1_neurons\n",
      "128               |80                |task_0_layer_2_neurons\n",
      "8                 |88                |task_0_layer_3_neurons\n",
      "24                |72                |task_0_layer_4_neurons\n",
      "96                |72                |task_0_layer_5_neurons\n",
      "96                |80                |task_0_layer_6_neurons\n",
      "96                |120               |task_9_layer_0_neurons\n",
      "16                |24                |task_9_layer_1_neurons\n",
      "40                |56                |task_9_layer_2_neurons\n",
      "40                |80                |task_9_layer_3_neurons\n",
      "112               |88                |task_9_layer_4_neurons\n",
      "56                |104               |task_3_layer_1_neurons\n",
      "56                |48                |task_3_layer_2_neurons\n",
      "80                |128               |task_13_layer_4_neurons\n",
      "104               |120               |task_13_layer_5_neurons\n",
      "16                |80                |task_13_layer_6_neurons\n",
      "112               |120               |task_13_layer_7_neurons\n",
      "40                |72                |task_13_layer_8_neurons\n",
      "16                |24                |task_13_layer_9_neurons\n",
      "72                |96                |task_9_layer_5_neurons\n",
      "40                |120               |task_9_layer_6_neurons\n",
      "16                |40                |task_9_layer_7_neurons\n",
      "80                |120               |task_9_layer_8_neurons\n",
      "8                 |72                |task_9_layer_9_neurons\n",
      "56                |128               |task_8_layer_0_neurons\n",
      "96                |64                |task_8_layer_1_neurons\n",
      "48                |8                 |task_8_layer_2_neurons\n",
      "48                |48                |task_8_layer_3_neurons\n",
      "80                |48                |task_8_layer_4_neurons\n",
      "24                |112               |task_4_layer_0_neurons\n",
      "72                |32                |task_4_layer_1_neurons\n",
      "64                |128               |task_4_layer_2_neurons\n",
      "96                |40                |task_4_layer_3_neurons\n",
      "104               |40                |task_4_layer_4_neurons\n",
      "96                |72                |task_4_layer_5_neurons\n",
      "56                |96                |task_4_layer_6_neurons\n",
      "32                |48                |task_4_layer_7_neurons\n",
      "16                |64                |task_4_layer_8_neurons\n",
      "2                 |7                 |task_1_num_layers\n",
      "1                 |1                 |task_12_num_layers\n",
      "128               |48                |task_7_layer_5_neurons\n",
      "88                |56                |task_7_layer_6_neurons\n",
      "72                |32                |task_12_layer_0_neurons\n",
      "56                |120               |task_12_layer_1_neurons\n",
      "96                |72                |task_12_layer_2_neurons\n",
      "8                 |64                |task_12_layer_3_neurons\n",
      "104               |120               |task_12_layer_4_neurons\n",
      "72                |32                |task_12_layer_5_neurons\n",
      "72                |64                |task_12_layer_6_neurons\n",
      "56                |88                |task_12_layer_7_neurons\n",
      "80                |56                |task_12_layer_8_neurons\n",
      "16                |64                |task_8_layer_5_neurons\n",
      "56                |128               |task_1_layer_0_neurons\n",
      "24                |48                |task_1_layer_1_neurons\n",
      "64                |8                 |task_1_layer_2_neurons\n",
      "24                |104               |task_1_layer_3_neurons\n",
      "32                |16                |task_1_layer_4_neurons\n",
      "32                |56                |task_1_layer_5_neurons\n",
      "24                |80                |task_1_layer_6_neurons\n",
      "120               |88                |task_1_layer_7_neurons\n",
      "3                 |7                 |task_10_num_layers\n",
      "120               |128               |task_4_layer_9_neurons\n",
      "56                |104               |task_7_layer_7_neurons\n",
      "64                |128               |task_7_layer_8_neurons\n",
      "48                |16                |task_7_layer_9_neurons\n",
      "96                |112               |task_3_layer_3_neurons\n",
      "40                |96                |task_3_layer_4_neurons\n",
      "112               |48                |task_3_layer_5_neurons\n",
      "48                |16                |task_3_layer_6_neurons\n",
      "128               |48                |task_3_layer_7_neurons\n",
      "120               |128               |task_3_layer_8_neurons\n",
      "48                |40                |task_6_layer_7_neurons\n",
      "120               |16                |task_6_layer_8_neurons\n",
      "128               |80                |task_6_layer_9_neurons\n",
      "16                |112               |task_10_layer_0_neurons\n",
      "120               |88                |task_10_layer_1_neurons\n",
      "8                 |80                |task_10_layer_2_neurons\n",
      "128               |64                |task_10_layer_3_neurons\n",
      "24                |112               |task_10_layer_4_neurons\n",
      "128               |8                 |task_10_layer_5_neurons\n",
      "120               |104               |task_10_layer_6_neurons\n",
      "16                |80                |task_10_layer_7_neurons\n",
      "8                 |120               |task_10_layer_8_neurons\n",
      "112               |128               |task_10_layer_9_neurons\n",
      "104               |24                |task_3_layer_9_neurons\n",
      "88                |112               |task_2_layer_8_neurons\n",
      "112               |112               |task_2_layer_9_neurons\n",
      "96                |8                 |task_0_layer_7_neurons\n",
      "64                |120               |task_0_layer_8_neurons\n",
      "128               |8                 |task_11_layer_9_neurons\n",
      "64                |80                |task_1_layer_8_neurons\n",
      "72                |80                |task_1_layer_9_neurons\n",
      "96                |80                |task_8_layer_6_neurons\n",
      "96                |120               |task_12_layer_9_neurons\n",
      "48                |40                |task_14_layer_8_neurons\n",
      "120               |96                |task_14_layer_9_neurons\n",
      "104               |72                |task_8_layer_7_neurons\n",
      "48                |88                |task_8_layer_8_neurons\n",
      "80                |56                |task_0_layer_9_neurons\n",
      "48                |48                |task_8_layer_9_neurons\n",
      "18                |18                |tuner/epochs\n",
      "9                 |9                 |tuner/initial_epoch\n",
      "10                |10                |tuner/bracket\n",
      "4                 |4                 |tuner/round\n",
      "2034              |1957              |tuner/trial_id\n",
      "\n",
      "Epoch 10/18\n",
      "17/17 [==============================] - 9s 40ms/step - loss: 0.0631 - y1_loss: 0.0554 - y2_loss: 0.0484 - y3_loss: 0.0020 - y1_mae: 0.1564 - y2_mae: 0.1575 - y3_mae: 0.0020 - val_loss: 0.0952 - val_y1_loss: 0.0606 - val_y2_loss: 0.0971 - val_y3_loss: 0.0015 - val_y1_mae: 0.1662 - val_y2_mae: 0.2164 - val_y3_mae: 0.0015\n",
      "Epoch 11/18\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0539 - y1_loss: 0.0435 - y2_loss: 0.0447 - y3_loss: 0.0026 - y1_mae: 0.1402 - y2_mae: 0.1509 - y3_mae: 0.0025 - val_loss: 0.0940 - val_y1_loss: 0.0565 - val_y2_loss: 0.0995 - val_y3_loss: 9.8305e-04 - val_y1_mae: 0.1663 - val_y2_mae: 0.2081 - val_y3_mae: 9.7956e-04\n",
      "Epoch 12/18\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0499 - y1_loss: 0.0357 - y2_loss: 0.0468 - y3_loss: 9.1656e-04 - y1_mae: 0.1276 - y2_mae: 0.1531 - y3_mae: 9.1449e-04 - val_loss: 0.0840 - val_y1_loss: 0.0658 - val_y2_loss: 0.0735 - val_y3_loss: 0.0010 - val_y1_mae: 0.1698 - val_y2_mae: 0.1845 - val_y3_mae: 0.0010\n",
      "Epoch 13/18\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.0410 - y1_loss: 0.0310 - y2_loss: 0.0368 - y3_loss: 8.7727e-04 - y1_mae: 0.1181 - y2_mae: 0.1390 - y3_mae: 8.7348e-04 - val_loss: 0.0959 - val_y1_loss: 0.0749 - val_y2_loss: 0.0843 - val_y3_loss: 8.0079e-04 - val_y1_mae: 0.1910 - val_y2_mae: 0.1886 - val_y3_mae: 7.9616e-04\n",
      "Epoch 14/18\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.0457 - y1_loss: 0.0356 - y2_loss: 0.0400 - y3_loss: 9.5048e-04 - y1_mae: 0.1295 - y2_mae: 0.1376 - y3_mae: 9.4386e-04 - val_loss: 0.0832 - val_y1_loss: 0.0651 - val_y2_loss: 0.0730 - val_y3_loss: 8.1220e-04 - val_y1_mae: 0.1703 - val_y2_mae: 0.1721 - val_y3_mae: 8.0583e-04\n",
      "Epoch 15/18\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0412 - y1_loss: 0.0353 - y2_loss: 0.0328 - y3_loss: 9.6311e-04 - y1_mae: 0.1263 - y2_mae: 0.1307 - y3_mae: 9.4651e-04 - val_loss: 0.0884 - val_y1_loss: 0.0831 - val_y2_loss: 0.0637 - val_y3_loss: 7.6722e-04 - val_y1_mae: 0.1814 - val_y2_mae: 0.1723 - val_y3_mae: 7.6458e-04\n",
      "Epoch 16/18\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.0403 - y1_loss: 0.0334 - y2_loss: 0.0332 - y3_loss: 6.7563e-04 - y1_mae: 0.1233 - y2_mae: 0.1266 - y3_mae: 6.7374e-04 - val_loss: 0.0818 - val_y1_loss: 0.0573 - val_y2_loss: 0.0786 - val_y3_loss: 5.6128e-04 - val_y1_mae: 0.1550 - val_y2_mae: 0.1953 - val_y3_mae: 5.5995e-04\n",
      "Epoch 17/18\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.0422 - y1_loss: 0.0359 - y2_loss: 0.0340 - y3_loss: 5.4420e-04 - y1_mae: 0.1173 - y2_mae: 0.1300 - y3_mae: 5.4334e-04"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "# Define custom loss function\n",
    "\n",
    "\n",
    "# Define model builder function for Keras Tuner\n",
    "def model_builder(hp):\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Define model\n",
    "    inputs = tf.keras.Input(shape=(10,))\n",
    "    shared_layer = inputs\n",
    "    for i in range(hp.Int('num_layers', 1, 15)):\n",
    "        shared_layer = layers.Dense(\n",
    "            units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "            activation='relu',\n",
    "            # add elu\n",
    "            kernel_initializer='he_uniform'\n",
    "        )(shared_layer)\n",
    "\n",
    "    task_layer1 = shared_layer\n",
    "    for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "        task_layer1 = layers.Dense(units=hp.Int(f'task_{i}_layer_{j}_neurons', min_value=8, max_value=256, step=8), \n",
    "                                    activation='relu',\n",
    "                                    kernel_initializer='he_uniform')(task_layer1)\n",
    "    out1 = layers.Dense(2, name='y1', activation = 'linear', kernel_initializer='he_uniform')(task_layer1)\n",
    "    out2 = layers.Dense(2, name='y2', activation = 'linear', kernel_initializer='he_uniform')(task_layer1)\n",
    "\n",
    "    task_layer2 = shared_layer\n",
    "    for j in range(hp.Int(f'task_{i}_num_layers', 0, 10)):\n",
    "        task_layer2 = layers.Dense(units=hp.Int(f'task_{i}_layer_{j}_neurons', min_value=2, max_value=64, step=2),\n",
    "                                    activation='relu',\n",
    "                                    kernel_initializer='he_uniform')(task_layer2)\n",
    "    out3 = layers.Dense(1, name='y3', activation = 'sigmoid', kernel_initializer='he_uniform')(task_layer2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[out1, out2, out3])\n",
    "\n",
    "    loss1_weight = hp.Choice('loss1_weight', values=[0.6, 0.7, 0.8, 0.9])\n",
    "    loss2_weight = 1 - loss1_weight\n",
    "    lossWeights = {\"y1\": loss1_weight, \"y2\": loss1_weight, \"y3\": loss2_weight}\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-1, sampling=\"log\")\n",
    "\n",
    "    # model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss=['mse', masked_mse])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate),\n",
    "                    loss=losses, \n",
    "                    loss_weights=lossWeights,\n",
    "                    metrics = metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    model_builder,\n",
    "    objective =  kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_epochs=EPOCHS+100,\n",
    "    factor=2,\n",
    "    directory=\"../../tensorflow_log_files/studienarbeit/\",\n",
    "    project_name=project_name,\n",
    "    seed = 0\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(train_dataset.map(lambda x, y1, y2, y3: (x, {'y1': y1, 'y2': y2, 'y3': y3})),\n",
    "            #  X_train, y_train, \n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2, y3: (x, {'y1': y1, 'y2': y2, 'y3': y3})), \n",
    "             verbose = 1, callbacks=[stop_early],\n",
    "             epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\", best_hps)\n",
    "\n",
    "# Build the best model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "history = best_model.fit(train_dataset.map(lambda x, y1, y2, y3: (x, {'y1': y1, 'y2': y2, 'y3': y3})),\n",
    "            #  X_train, y_train,\n",
    "            #  validation_data = (X_val, y_val), \n",
    "            validation_data = val_dataset.map(lambda x, y1, y2, y3: (x, {'y1': y1, 'y2': y2, 'y3': y3})), \n",
    "            callbacks=[stop_early],\n",
    "            verbose = 1, epochs=EPOCHS, shuffle = True)\n",
    "\n",
    "print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
    "    is {best_model.optimizer.lr.numpy()}.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(model_path)\n",
    "best_model = tf.keras.models.load_model(model_path)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_predictions_train = best_model.predict(train_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "# print(\"train\", \"{:10.4f}\".format(mean_squared_error(y_train, y_predictions, squared=True)))\n",
    "y_predictions_val = best_model.predict(val_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "# print(\"val\", \"{:10.4f}\".format(mean_squared_error(y_val, y_predictions, squared=True)))\n",
    "y_predictions = best_model.predict(test_dataset.map(lambda x, y1: (x, {'y1': y1})))\n",
    "\n",
    "y_predictions_train = np.concatenate((y_predictions_train[0], y_predictions_train[1]), axis=1)\n",
    "y_predictions_val = np.concatenate((y_predictions_val[0], y_predictions_val[1]), axis=1)\n",
    "y_predictions = np.concatenate((y_predictions[0], y_predictions[1]), axis=1)\n",
    "\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) < 0.5] = 0\n",
    "y_predictions_train[:,-1][np.abs(y_predictions_train[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) < 0.5] = 0\n",
    "y_predictions_val[:,-1][np.abs(y_predictions_val[:,-1]) > 0.5] = 1\n",
    "\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) < 0.5] = 0\n",
    "y_predictions[:,-1][np.abs(y_predictions[:,-1]) > 0.5] = 1\n",
    "\n",
    "loss_test = \"{:10.4f}\".format(mean_squared_error(y_test_all, y_predictions, squared=True))\n",
    "metric_test = \"{:10.4f}\".format(mean_absolute_error(y_test_all, y_predictions))\n",
    "\n",
    "loss_val = \"{:10.4f}\".format(mean_squared_error(y_val_all, y_predictions_val, squared=True))\n",
    "metric_val = \"{:10.4f}\".format(mean_absolute_error(y_val_all, y_predictions_val))\n",
    "\n",
    "loss_train = \"{:10.4f}\".format(mean_squared_error(y_train_all, y_predictions_train, squared=True))\n",
    "metric_train = \"{:10.4f}\".format(mean_absolute_error(y_train_all, y_predictions_train))\n",
    "\n",
    "print(metric_test, metric_val, metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = best_model.evaluate(train_dataset.map(lambda x, y1: (x, {'y1': y1})), verbose=1)\n",
    "results_val = best_model.evaluate(val_dataset.map(lambda x, y1: (x, {'y1': y1})), verbose=1)\n",
    "results_test = best_model.evaluate(test_dataset.map(lambda x, y1: (x, {'y1': y1,})), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studiarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
